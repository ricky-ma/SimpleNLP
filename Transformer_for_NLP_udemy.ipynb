{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_for_NLP_udemy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricky-ma/UdemyNLP/blob/master/Transformer_for_NLP_udemy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9JJ7FBw84tG",
        "colab_type": "text"
      },
      "source": [
        "# Stage 1: Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbcvtPlp3YWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6o_cpZz3y_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQN8jwx48_yU",
        "colab_type": "text"
      },
      "source": [
        "# Stage 2: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPlOT-2mlw0r",
        "colab_type": "text"
      },
      "source": [
        "## Loading files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCD9jwXsLwS_",
        "colab_type": "text"
      },
      "source": [
        "We import files from our personal google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQpbl1pXCR0p",
        "colab_type": "code",
        "outputId": "455929ae-042b-4fa9-8940-34f2a17b5a0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Or0sLV5b8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/Work/UdemyNLP/europarl-v7.fr-en.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_en = f.read()\n",
        "with open(\"/content/drive/My Drive/Work/UdemyNLP/europarl-v7.fr-en.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_fr = f.read()\n",
        "with open(\"/content/drive/My Drive/Work/UdemyNLP/nonbreaking_prefix.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/My Drive/Work/UdemyNLP/nonbreaking_prefix.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_fr = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEFw0D2vP_Dl",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwIBeGXn7LIJ",
        "colab_type": "text"
      },
      "source": [
        "Getting the non_breaking_prefixes as a clean list of words with a point at the end so it is easier to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_TeuktU40Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9x4mZfKMaxD",
        "colab_type": "text"
      },
      "source": [
        "We will need each word and other symbol that we want to keep to be in lower case and separated by spaces so we can \"tokenize\" them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg-8LLK-WdFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en\n",
        "# Add $$$ after non ending sentence points\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "# Remove $$$ markers\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "# Clear multiple spaces\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
        "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
        "corpus_fr = corpus_fr.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Y9v8-Tozl2",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5YXanmOd_xK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size=2**13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftIbPzIwCtwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPFe2YJDC9jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
        "           for sentence in corpus_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG6AlcFMpC5C",
        "colab_type": "text"
      },
      "source": [
        "## Remove too long sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6CD6PLGyQWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypm8h5aZQTZ1",
        "colab_type": "text"
      },
      "source": [
        "## Inputs/outputs creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FP0WPsdM8hl",
        "colab_type": "text"
      },
      "source": [
        "As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvDfLDWUONlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFxMp3TOIYff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycT0YqydRcUd",
        "colab_type": "text"
      },
      "source": [
        "# Stage 3: Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SBoH8G4XyR9",
        "colab_type": "text"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9C3ucmJ86I",
        "colab_type": "text"
      },
      "source": [
        "Positional encoding formulae:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2wc6sYlX0dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcw8YIQqRhOJ",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sffhwwvX-wj",
        "colab_type": "text"
      },
      "source": [
        "### Attention computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBuW6lESLDX",
        "colab_type": "text"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rEoCNJURbrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MjtvXrfYEx7",
        "colab_type": "text"
      },
      "source": [
        "### Multi-head attention sublayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvq4I9uTX5p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiyuHe1OeT5N",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV0ZMH7KT_KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units          # number of feed-forward units\n",
        "        self.nb_proj = nb_proj              # number of projections\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs, inputs,\n",
        "                                              inputs, mask)\n",
        "        attention = self.dropout_1(attention, training=training)  # dropout to avoid overfitting\n",
        "        attention = self.norm_1(attention + inputs)               # layer normalization\n",
        "        \n",
        "        outputs = self.dense_1(attention)                         # first part of FFN\n",
        "        outputs = self.dense_2(outputs)                           # second part of FFN\n",
        "        outputs = self.dropout_2(outputs, training=training)      # dropout to avoid overfitting\n",
        "        outputs = self.norm_2(outputs + attention)                # layer normalization\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-P92KeZih60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    # building the encoder\n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)                            # apply embedding layer\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # normalization\n",
        "        outputs = self.pos_encoding(outputs)                        # apply position encoding\n",
        "        outputs = self.dropout(outputs, training)                   # dropout to avoid overfitting\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)   # apply encoding layers nb_layers times\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DthraBEwuvl",
        "colab_type": "text"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZWZyFBnwy8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combined with encoder output\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpzdiWHiwywF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for i in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5sJYkjbz5DD",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqvqNjJPwyh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c-LRThUPrso",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiOdqQ5qPs8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46xg4Wrg1Wgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Goque362343",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb_32PIU5Zkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/projects/transformer/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhFK5kUx602K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed03b177-cc5f-46f7-fd98-649d6435ce26"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 6.0344 Accuracy 0.0008\n",
            "Epoch 1 Batch 50 Loss 6.3076 Accuracy 0.0193\n",
            "Epoch 1 Batch 100 Loss 6.2290 Accuracy 0.0358\n",
            "Epoch 1 Batch 150 Loss 6.1376 Accuracy 0.0414\n",
            "Epoch 1 Batch 200 Loss 6.0539 Accuracy 0.0442\n",
            "Epoch 1 Batch 250 Loss 5.9482 Accuracy 0.0458\n",
            "Epoch 1 Batch 300 Loss 5.8178 Accuracy 0.0470\n",
            "Epoch 1 Batch 350 Loss 5.6910 Accuracy 0.0498\n",
            "Epoch 1 Batch 400 Loss 5.5602 Accuracy 0.0548\n",
            "Epoch 1 Batch 450 Loss 5.4411 Accuracy 0.0598\n",
            "Epoch 1 Batch 500 Loss 5.3369 Accuracy 0.0649\n",
            "Epoch 1 Batch 550 Loss 5.2276 Accuracy 0.0697\n",
            "Epoch 1 Batch 600 Loss 5.1298 Accuracy 0.0750\n",
            "Epoch 1 Batch 650 Loss 5.0368 Accuracy 0.0806\n",
            "Epoch 1 Batch 700 Loss 4.9507 Accuracy 0.0860\n",
            "Epoch 1 Batch 750 Loss 4.8670 Accuracy 0.0913\n",
            "Epoch 1 Batch 800 Loss 4.7838 Accuracy 0.0962\n",
            "Epoch 1 Batch 850 Loss 4.7071 Accuracy 0.1012\n",
            "Epoch 1 Batch 900 Loss 4.6354 Accuracy 0.1060\n",
            "Epoch 1 Batch 950 Loss 4.5676 Accuracy 0.1108\n",
            "Epoch 1 Batch 1000 Loss 4.5007 Accuracy 0.1151\n",
            "Epoch 1 Batch 1050 Loss 4.4399 Accuracy 0.1192\n",
            "Epoch 1 Batch 1100 Loss 4.3826 Accuracy 0.1231\n",
            "Epoch 1 Batch 1150 Loss 4.3292 Accuracy 0.1268\n",
            "Epoch 1 Batch 1200 Loss 4.2784 Accuracy 0.1303\n",
            "Epoch 1 Batch 1250 Loss 4.2301 Accuracy 0.1337\n",
            "Epoch 1 Batch 1300 Loss 4.1842 Accuracy 0.1370\n",
            "Epoch 1 Batch 1350 Loss 4.1395 Accuracy 0.1403\n",
            "Epoch 1 Batch 1400 Loss 4.0959 Accuracy 0.1435\n",
            "Epoch 1 Batch 1450 Loss 4.0558 Accuracy 0.1467\n",
            "Epoch 1 Batch 1500 Loss 4.0163 Accuracy 0.1498\n",
            "Epoch 1 Batch 1550 Loss 3.9786 Accuracy 0.1528\n",
            "Epoch 1 Batch 1600 Loss 3.9427 Accuracy 0.1558\n",
            "Epoch 1 Batch 1650 Loss 3.9067 Accuracy 0.1586\n",
            "Epoch 1 Batch 1700 Loss 3.8733 Accuracy 0.1614\n",
            "Epoch 1 Batch 1750 Loss 3.8413 Accuracy 0.1641\n",
            "Epoch 1 Batch 1800 Loss 3.8109 Accuracy 0.1667\n",
            "Epoch 1 Batch 1850 Loss 3.7812 Accuracy 0.1693\n",
            "Epoch 1 Batch 1900 Loss 3.7532 Accuracy 0.1718\n",
            "Epoch 1 Batch 1950 Loss 3.7250 Accuracy 0.1743\n",
            "Epoch 1 Batch 2000 Loss 3.6976 Accuracy 0.1766\n",
            "Epoch 1 Batch 2050 Loss 3.6709 Accuracy 0.1788\n",
            "Epoch 1 Batch 2100 Loss 3.6443 Accuracy 0.1808\n",
            "Epoch 1 Batch 2150 Loss 3.6174 Accuracy 0.1828\n",
            "Epoch 1 Batch 2200 Loss 3.5902 Accuracy 0.1847\n",
            "Epoch 1 Batch 2250 Loss 3.5642 Accuracy 0.1865\n",
            "Epoch 1 Batch 2300 Loss 3.5382 Accuracy 0.1884\n",
            "Epoch 1 Batch 2350 Loss 3.5132 Accuracy 0.1904\n",
            "Epoch 1 Batch 2400 Loss 3.4887 Accuracy 0.1923\n",
            "Epoch 1 Batch 2450 Loss 3.4645 Accuracy 0.1943\n",
            "Epoch 1 Batch 2500 Loss 3.4416 Accuracy 0.1962\n",
            "Epoch 1 Batch 2550 Loss 3.4188 Accuracy 0.1982\n",
            "Epoch 1 Batch 2600 Loss 3.3967 Accuracy 0.2002\n",
            "Epoch 1 Batch 2650 Loss 3.3754 Accuracy 0.2021\n",
            "Epoch 1 Batch 2700 Loss 3.3531 Accuracy 0.2040\n",
            "Epoch 1 Batch 2750 Loss 3.3310 Accuracy 0.2060\n",
            "Epoch 1 Batch 2800 Loss 3.3103 Accuracy 0.2079\n",
            "Epoch 1 Batch 2850 Loss 3.2898 Accuracy 0.2098\n",
            "Epoch 1 Batch 2900 Loss 3.2698 Accuracy 0.2117\n",
            "Epoch 1 Batch 2950 Loss 3.2502 Accuracy 0.2136\n",
            "Epoch 1 Batch 3000 Loss 3.2313 Accuracy 0.2154\n",
            "Epoch 1 Batch 3050 Loss 3.2122 Accuracy 0.2172\n",
            "Epoch 1 Batch 3100 Loss 3.1938 Accuracy 0.2190\n",
            "Epoch 1 Batch 3150 Loss 3.1753 Accuracy 0.2208\n",
            "Epoch 1 Batch 3200 Loss 3.1571 Accuracy 0.2226\n",
            "Epoch 1 Batch 3250 Loss 3.1386 Accuracy 0.2245\n",
            "Epoch 1 Batch 3300 Loss 3.1205 Accuracy 0.2264\n",
            "Epoch 1 Batch 3350 Loss 3.1029 Accuracy 0.2281\n",
            "Epoch 1 Batch 3400 Loss 3.0858 Accuracy 0.2300\n",
            "Epoch 1 Batch 3450 Loss 3.0690 Accuracy 0.2318\n",
            "Epoch 1 Batch 3500 Loss 3.0517 Accuracy 0.2336\n",
            "Epoch 1 Batch 3550 Loss 3.0353 Accuracy 0.2354\n",
            "Epoch 1 Batch 3600 Loss 3.0182 Accuracy 0.2371\n",
            "Epoch 1 Batch 3650 Loss 3.0027 Accuracy 0.2388\n",
            "Epoch 1 Batch 3700 Loss 2.9872 Accuracy 0.2405\n",
            "Epoch 1 Batch 3750 Loss 2.9718 Accuracy 0.2422\n",
            "Epoch 1 Batch 3800 Loss 2.9567 Accuracy 0.2439\n",
            "Epoch 1 Batch 3850 Loss 2.9420 Accuracy 0.2456\n",
            "Epoch 1 Batch 3900 Loss 2.9276 Accuracy 0.2473\n",
            "Epoch 1 Batch 3950 Loss 2.9132 Accuracy 0.2490\n",
            "Epoch 1 Batch 4000 Loss 2.8984 Accuracy 0.2507\n",
            "Epoch 1 Batch 4050 Loss 2.8849 Accuracy 0.2524\n",
            "Epoch 1 Batch 4100 Loss 2.8714 Accuracy 0.2540\n",
            "Epoch 1 Batch 4150 Loss 2.8588 Accuracy 0.2554\n",
            "Epoch 1 Batch 4200 Loss 2.8465 Accuracy 0.2568\n",
            "Epoch 1 Batch 4250 Loss 2.8345 Accuracy 0.2581\n",
            "Epoch 1 Batch 4300 Loss 2.8237 Accuracy 0.2594\n",
            "Epoch 1 Batch 4350 Loss 2.8128 Accuracy 0.2606\n",
            "Epoch 1 Batch 4400 Loss 2.8019 Accuracy 0.2618\n",
            "Epoch 1 Batch 4450 Loss 2.7917 Accuracy 0.2630\n",
            "Epoch 1 Batch 4500 Loss 2.7813 Accuracy 0.2641\n",
            "Epoch 1 Batch 4550 Loss 2.7715 Accuracy 0.2652\n",
            "Epoch 1 Batch 4600 Loss 2.7617 Accuracy 0.2663\n",
            "Epoch 1 Batch 4650 Loss 2.7520 Accuracy 0.2673\n",
            "Epoch 1 Batch 4700 Loss 2.7425 Accuracy 0.2684\n",
            "Epoch 1 Batch 4750 Loss 2.7329 Accuracy 0.2695\n",
            "Epoch 1 Batch 4800 Loss 2.7231 Accuracy 0.2705\n",
            "Epoch 1 Batch 4850 Loss 2.7133 Accuracy 0.2716\n",
            "Epoch 1 Batch 4900 Loss 2.7038 Accuracy 0.2726\n",
            "Epoch 1 Batch 4950 Loss 2.6945 Accuracy 0.2736\n",
            "Epoch 1 Batch 5000 Loss 2.6853 Accuracy 0.2746\n",
            "Epoch 1 Batch 5050 Loss 2.6764 Accuracy 0.2757\n",
            "Epoch 1 Batch 5100 Loss 2.6676 Accuracy 0.2766\n",
            "Epoch 1 Batch 5150 Loss 2.6588 Accuracy 0.2776\n",
            "Epoch 1 Batch 5200 Loss 2.6502 Accuracy 0.2785\n",
            "Epoch 1 Batch 5250 Loss 2.6416 Accuracy 0.2794\n",
            "Epoch 1 Batch 5300 Loss 2.6328 Accuracy 0.2803\n",
            "Epoch 1 Batch 5350 Loss 2.6243 Accuracy 0.2812\n",
            "Epoch 1 Batch 5400 Loss 2.6156 Accuracy 0.2820\n",
            "Epoch 1 Batch 5450 Loss 2.6071 Accuracy 0.2829\n",
            "Epoch 1 Batch 5500 Loss 2.5989 Accuracy 0.2838\n",
            "Epoch 1 Batch 5550 Loss 2.5906 Accuracy 0.2846\n",
            "Epoch 1 Batch 5600 Loss 2.5822 Accuracy 0.2855\n",
            "Epoch 1 Batch 5650 Loss 2.5744 Accuracy 0.2863\n",
            "Epoch 1 Batch 5700 Loss 2.5665 Accuracy 0.2871\n",
            "Saving checkpoint for epoch 1 at ./drive/My Drive/projects/transformer/ckpt/ckpt-1\n",
            "Time taken for 1 epoch: 2087.0132069587708 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.9478 Accuracy 0.3709\n",
            "Epoch 2 Batch 50 Loss 1.6891 Accuracy 0.3843\n",
            "Epoch 2 Batch 100 Loss 1.6887 Accuracy 0.3837\n",
            "Epoch 2 Batch 150 Loss 1.6843 Accuracy 0.3858\n",
            "Epoch 2 Batch 200 Loss 1.6803 Accuracy 0.3871\n",
            "Epoch 2 Batch 250 Loss 1.6693 Accuracy 0.3881\n",
            "Epoch 2 Batch 300 Loss 1.6587 Accuracy 0.3895\n",
            "Epoch 2 Batch 350 Loss 1.6595 Accuracy 0.3905\n",
            "Epoch 2 Batch 400 Loss 1.6508 Accuracy 0.3908\n",
            "Epoch 2 Batch 450 Loss 1.6429 Accuracy 0.3917\n",
            "Epoch 2 Batch 500 Loss 1.6405 Accuracy 0.3920\n",
            "Epoch 2 Batch 550 Loss 1.6345 Accuracy 0.3922\n",
            "Epoch 2 Batch 600 Loss 1.6317 Accuracy 0.3927\n",
            "Epoch 2 Batch 650 Loss 1.6305 Accuracy 0.3932\n",
            "Epoch 2 Batch 700 Loss 1.6264 Accuracy 0.3939\n",
            "Epoch 2 Batch 750 Loss 1.6237 Accuracy 0.3942\n",
            "Epoch 2 Batch 800 Loss 1.6210 Accuracy 0.3946\n",
            "Epoch 2 Batch 850 Loss 1.6183 Accuracy 0.3953\n",
            "Epoch 2 Batch 900 Loss 1.6136 Accuracy 0.3954\n",
            "Epoch 2 Batch 950 Loss 1.6106 Accuracy 0.3960\n",
            "Epoch 2 Batch 1000 Loss 1.6060 Accuracy 0.3961\n",
            "Epoch 2 Batch 1050 Loss 1.6033 Accuracy 0.3964\n",
            "Epoch 2 Batch 1100 Loss 1.6018 Accuracy 0.3967\n",
            "Epoch 2 Batch 1150 Loss 1.5985 Accuracy 0.3972\n",
            "Epoch 2 Batch 1200 Loss 1.5951 Accuracy 0.3975\n",
            "Epoch 2 Batch 1250 Loss 1.5898 Accuracy 0.3979\n",
            "Epoch 2 Batch 1300 Loss 1.5867 Accuracy 0.3988\n",
            "Epoch 2 Batch 1350 Loss 1.5828 Accuracy 0.3994\n",
            "Epoch 2 Batch 1400 Loss 1.5795 Accuracy 0.4002\n",
            "Epoch 2 Batch 1450 Loss 1.5760 Accuracy 0.4010\n",
            "Epoch 2 Batch 1500 Loss 1.5715 Accuracy 0.4020\n",
            "Epoch 2 Batch 1550 Loss 1.5670 Accuracy 0.4030\n",
            "Epoch 2 Batch 1600 Loss 1.5631 Accuracy 0.4039\n",
            "Epoch 2 Batch 1650 Loss 1.5588 Accuracy 0.4050\n",
            "Epoch 2 Batch 1700 Loss 1.5545 Accuracy 0.4059\n",
            "Epoch 2 Batch 1750 Loss 1.5511 Accuracy 0.4067\n",
            "Epoch 2 Batch 1800 Loss 1.5470 Accuracy 0.4076\n",
            "Epoch 2 Batch 1850 Loss 1.5434 Accuracy 0.4086\n",
            "Epoch 2 Batch 1900 Loss 1.5396 Accuracy 0.4095\n",
            "Epoch 2 Batch 1950 Loss 1.5357 Accuracy 0.4105\n",
            "Epoch 2 Batch 2000 Loss 1.5324 Accuracy 0.4113\n",
            "Epoch 2 Batch 2050 Loss 1.5286 Accuracy 0.4120\n",
            "Epoch 2 Batch 2100 Loss 1.5252 Accuracy 0.4126\n",
            "Epoch 2 Batch 2150 Loss 1.5207 Accuracy 0.4131\n",
            "Epoch 2 Batch 2200 Loss 1.5158 Accuracy 0.4136\n",
            "Epoch 2 Batch 2250 Loss 1.5109 Accuracy 0.4140\n",
            "Epoch 2 Batch 2300 Loss 1.5056 Accuracy 0.4145\n",
            "Epoch 2 Batch 2350 Loss 1.5007 Accuracy 0.4150\n",
            "Epoch 2 Batch 2400 Loss 1.4966 Accuracy 0.4155\n",
            "Epoch 2 Batch 2450 Loss 1.4919 Accuracy 0.4160\n",
            "Epoch 2 Batch 2500 Loss 1.4874 Accuracy 0.4166\n",
            "Epoch 2 Batch 2550 Loss 1.4833 Accuracy 0.4172\n",
            "Epoch 2 Batch 2600 Loss 1.4786 Accuracy 0.4176\n",
            "Epoch 2 Batch 2650 Loss 1.4736 Accuracy 0.4183\n",
            "Epoch 2 Batch 2700 Loss 1.4693 Accuracy 0.4189\n",
            "Epoch 2 Batch 2750 Loss 1.4653 Accuracy 0.4194\n",
            "Epoch 2 Batch 2800 Loss 1.4608 Accuracy 0.4199\n",
            "Epoch 2 Batch 2850 Loss 1.4570 Accuracy 0.4205\n",
            "Epoch 2 Batch 2900 Loss 1.4537 Accuracy 0.4211\n",
            "Epoch 2 Batch 2950 Loss 1.4498 Accuracy 0.4215\n",
            "Epoch 2 Batch 3000 Loss 1.4462 Accuracy 0.4221\n",
            "Epoch 2 Batch 3050 Loss 1.4424 Accuracy 0.4226\n",
            "Epoch 2 Batch 3100 Loss 1.4385 Accuracy 0.4231\n",
            "Epoch 2 Batch 3150 Loss 1.4348 Accuracy 0.4237\n",
            "Epoch 2 Batch 3200 Loss 1.4309 Accuracy 0.4241\n",
            "Epoch 2 Batch 3250 Loss 1.4274 Accuracy 0.4246\n",
            "Epoch 2 Batch 3300 Loss 1.4238 Accuracy 0.4252\n",
            "Epoch 2 Batch 3350 Loss 1.4198 Accuracy 0.4257\n",
            "Epoch 2 Batch 3400 Loss 1.4160 Accuracy 0.4263\n",
            "Epoch 2 Batch 3450 Loss 1.4123 Accuracy 0.4268\n",
            "Epoch 2 Batch 3500 Loss 1.4088 Accuracy 0.4273\n",
            "Epoch 2 Batch 3550 Loss 1.4052 Accuracy 0.4279\n",
            "Epoch 2 Batch 3600 Loss 1.4020 Accuracy 0.4285\n",
            "Epoch 2 Batch 3650 Loss 1.3989 Accuracy 0.4291\n",
            "Epoch 2 Batch 3700 Loss 1.3956 Accuracy 0.4296\n",
            "Epoch 2 Batch 3750 Loss 1.3923 Accuracy 0.4302\n",
            "Epoch 2 Batch 3800 Loss 1.3896 Accuracy 0.4307\n",
            "Epoch 2 Batch 3850 Loss 1.3865 Accuracy 0.4312\n",
            "Epoch 2 Batch 3900 Loss 1.3839 Accuracy 0.4317\n",
            "Epoch 2 Batch 3950 Loss 1.3809 Accuracy 0.4322\n",
            "Epoch 2 Batch 4000 Loss 1.3781 Accuracy 0.4327\n",
            "Epoch 2 Batch 4050 Loss 1.3750 Accuracy 0.4332\n",
            "Epoch 2 Batch 4100 Loss 1.3724 Accuracy 0.4336\n",
            "Epoch 2 Batch 4150 Loss 1.3706 Accuracy 0.4340\n",
            "Epoch 2 Batch 4200 Loss 1.3691 Accuracy 0.4342\n",
            "Epoch 2 Batch 4250 Loss 1.3683 Accuracy 0.4343\n",
            "Epoch 2 Batch 4300 Loss 1.3678 Accuracy 0.4344\n",
            "Epoch 2 Batch 4350 Loss 1.3675 Accuracy 0.4345\n",
            "Epoch 2 Batch 4400 Loss 1.3675 Accuracy 0.4346\n",
            "Epoch 2 Batch 4450 Loss 1.3670 Accuracy 0.4347\n",
            "Epoch 2 Batch 4500 Loss 1.3670 Accuracy 0.4348\n",
            "Epoch 2 Batch 4550 Loss 1.3673 Accuracy 0.4347\n",
            "Epoch 2 Batch 4600 Loss 1.3674 Accuracy 0.4348\n",
            "Epoch 2 Batch 4650 Loss 1.3673 Accuracy 0.4348\n",
            "Epoch 2 Batch 4700 Loss 1.3676 Accuracy 0.4348\n",
            "Epoch 2 Batch 4750 Loss 1.3675 Accuracy 0.4348\n",
            "Epoch 2 Batch 4800 Loss 1.3678 Accuracy 0.4348\n",
            "Epoch 2 Batch 4850 Loss 1.3678 Accuracy 0.4349\n",
            "Epoch 2 Batch 4900 Loss 1.3676 Accuracy 0.4349\n",
            "Epoch 2 Batch 4950 Loss 1.3675 Accuracy 0.4349\n",
            "Epoch 2 Batch 5000 Loss 1.3673 Accuracy 0.4349\n",
            "Epoch 2 Batch 5050 Loss 1.3673 Accuracy 0.4349\n",
            "Epoch 2 Batch 5100 Loss 1.3672 Accuracy 0.4349\n",
            "Epoch 2 Batch 5150 Loss 1.3675 Accuracy 0.4348\n",
            "Epoch 2 Batch 5200 Loss 1.3674 Accuracy 0.4348\n",
            "Epoch 2 Batch 5250 Loss 1.3673 Accuracy 0.4347\n",
            "Epoch 2 Batch 5300 Loss 1.3669 Accuracy 0.4345\n",
            "Epoch 2 Batch 5350 Loss 1.3668 Accuracy 0.4344\n",
            "Epoch 2 Batch 5400 Loss 1.3665 Accuracy 0.4343\n",
            "Epoch 2 Batch 5450 Loss 1.3663 Accuracy 0.4343\n",
            "Epoch 2 Batch 5500 Loss 1.3660 Accuracy 0.4342\n",
            "Epoch 2 Batch 5550 Loss 1.3657 Accuracy 0.4341\n",
            "Epoch 2 Batch 5600 Loss 1.3657 Accuracy 0.4341\n",
            "Epoch 2 Batch 5650 Loss 1.3654 Accuracy 0.4341\n",
            "Epoch 2 Batch 5700 Loss 1.3651 Accuracy 0.4341\n",
            "Saving checkpoint for epoch 2 at ./drive/My Drive/projects/transformer/ckpt/ckpt-2\n",
            "Time taken for 1 epoch: 2045.4077398777008 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.4614 Accuracy 0.4770\n",
            "Epoch 3 Batch 50 Loss 1.3648 Accuracy 0.4332\n",
            "Epoch 3 Batch 100 Loss 1.3420 Accuracy 0.4372\n",
            "Epoch 3 Batch 150 Loss 1.3346 Accuracy 0.4400\n",
            "Epoch 3 Batch 200 Loss 1.3332 Accuracy 0.4399\n",
            "Epoch 3 Batch 250 Loss 1.3311 Accuracy 0.4398\n",
            "Epoch 3 Batch 300 Loss 1.3263 Accuracy 0.4388\n",
            "Epoch 3 Batch 350 Loss 1.3244 Accuracy 0.4388\n",
            "Epoch 3 Batch 400 Loss 1.3217 Accuracy 0.4385\n",
            "Epoch 3 Batch 450 Loss 1.3187 Accuracy 0.4386\n",
            "Epoch 3 Batch 500 Loss 1.3168 Accuracy 0.4386\n",
            "Epoch 3 Batch 550 Loss 1.3113 Accuracy 0.4386\n",
            "Epoch 3 Batch 600 Loss 1.3095 Accuracy 0.4391\n",
            "Epoch 3 Batch 650 Loss 1.3078 Accuracy 0.4394\n",
            "Epoch 3 Batch 700 Loss 1.3082 Accuracy 0.4395\n",
            "Epoch 3 Batch 750 Loss 1.3087 Accuracy 0.4396\n",
            "Epoch 3 Batch 800 Loss 1.3073 Accuracy 0.4400\n",
            "Epoch 3 Batch 850 Loss 1.3048 Accuracy 0.4403\n",
            "Epoch 3 Batch 900 Loss 1.3042 Accuracy 0.4405\n",
            "Epoch 3 Batch 950 Loss 1.3019 Accuracy 0.4407\n",
            "Epoch 3 Batch 1000 Loss 1.3010 Accuracy 0.4406\n",
            "Epoch 3 Batch 1050 Loss 1.2991 Accuracy 0.4407\n",
            "Epoch 3 Batch 1100 Loss 1.2973 Accuracy 0.4410\n",
            "Epoch 3 Batch 1150 Loss 1.2953 Accuracy 0.4414\n",
            "Epoch 3 Batch 1200 Loss 1.2946 Accuracy 0.4415\n",
            "Epoch 3 Batch 1250 Loss 1.2925 Accuracy 0.4418\n",
            "Epoch 3 Batch 1300 Loss 1.2905 Accuracy 0.4421\n",
            "Epoch 3 Batch 1350 Loss 1.2887 Accuracy 0.4428\n",
            "Epoch 3 Batch 1400 Loss 1.2850 Accuracy 0.4435\n",
            "Epoch 3 Batch 1450 Loss 1.2813 Accuracy 0.4443\n",
            "Epoch 3 Batch 1500 Loss 1.2775 Accuracy 0.4452\n",
            "Epoch 3 Batch 1550 Loss 1.2747 Accuracy 0.4460\n",
            "Epoch 3 Batch 1600 Loss 1.2714 Accuracy 0.4468\n",
            "Epoch 3 Batch 1650 Loss 1.2690 Accuracy 0.4477\n",
            "Epoch 3 Batch 1700 Loss 1.2669 Accuracy 0.4484\n",
            "Epoch 3 Batch 1750 Loss 1.2642 Accuracy 0.4493\n",
            "Epoch 3 Batch 1800 Loss 1.2617 Accuracy 0.4501\n",
            "Epoch 3 Batch 1850 Loss 1.2593 Accuracy 0.4510\n",
            "Epoch 3 Batch 1900 Loss 1.2570 Accuracy 0.4519\n",
            "Epoch 3 Batch 1950 Loss 1.2545 Accuracy 0.4528\n",
            "Epoch 3 Batch 2000 Loss 1.2522 Accuracy 0.4535\n",
            "Epoch 3 Batch 2050 Loss 1.2497 Accuracy 0.4540\n",
            "Epoch 3 Batch 2100 Loss 1.2475 Accuracy 0.4544\n",
            "Epoch 3 Batch 2150 Loss 1.2447 Accuracy 0.4547\n",
            "Epoch 3 Batch 2200 Loss 1.2412 Accuracy 0.4550\n",
            "Epoch 3 Batch 2250 Loss 1.2374 Accuracy 0.4551\n",
            "Epoch 3 Batch 2300 Loss 1.2344 Accuracy 0.4553\n",
            "Epoch 3 Batch 2350 Loss 1.2311 Accuracy 0.4555\n",
            "Epoch 3 Batch 2400 Loss 1.2275 Accuracy 0.4559\n",
            "Epoch 3 Batch 2450 Loss 1.2239 Accuracy 0.4562\n",
            "Epoch 3 Batch 2500 Loss 1.2206 Accuracy 0.4566\n",
            "Epoch 3 Batch 2550 Loss 1.2172 Accuracy 0.4570\n",
            "Epoch 3 Batch 2600 Loss 1.2139 Accuracy 0.4574\n",
            "Epoch 3 Batch 2650 Loss 1.2105 Accuracy 0.4579\n",
            "Epoch 3 Batch 2700 Loss 1.2075 Accuracy 0.4583\n",
            "Epoch 3 Batch 2750 Loss 1.2046 Accuracy 0.4586\n",
            "Epoch 3 Batch 2800 Loss 1.2022 Accuracy 0.4591\n",
            "Epoch 3 Batch 2850 Loss 1.1993 Accuracy 0.4594\n",
            "Epoch 3 Batch 2900 Loss 1.1968 Accuracy 0.4597\n",
            "Epoch 3 Batch 2950 Loss 1.1940 Accuracy 0.4601\n",
            "Epoch 3 Batch 3000 Loss 1.1916 Accuracy 0.4604\n",
            "Epoch 3 Batch 3050 Loss 1.1896 Accuracy 0.4608\n",
            "Epoch 3 Batch 3100 Loss 1.1872 Accuracy 0.4611\n",
            "Epoch 3 Batch 3150 Loss 1.1847 Accuracy 0.4614\n",
            "Epoch 3 Batch 3200 Loss 1.1816 Accuracy 0.4618\n",
            "Epoch 3 Batch 3250 Loss 1.1791 Accuracy 0.4621\n",
            "Epoch 3 Batch 3300 Loss 1.1764 Accuracy 0.4624\n",
            "Epoch 3 Batch 3350 Loss 1.1739 Accuracy 0.4628\n",
            "Epoch 3 Batch 3400 Loss 1.1715 Accuracy 0.4631\n",
            "Epoch 3 Batch 3450 Loss 1.1689 Accuracy 0.4635\n",
            "Epoch 3 Batch 3500 Loss 1.1667 Accuracy 0.4640\n",
            "Epoch 3 Batch 3550 Loss 1.1645 Accuracy 0.4644\n",
            "Epoch 3 Batch 3600 Loss 1.1624 Accuracy 0.4648\n",
            "Epoch 3 Batch 3650 Loss 1.1602 Accuracy 0.4652\n",
            "Epoch 3 Batch 3700 Loss 1.1578 Accuracy 0.4656\n",
            "Epoch 3 Batch 3750 Loss 1.1557 Accuracy 0.4660\n",
            "Epoch 3 Batch 3800 Loss 1.1535 Accuracy 0.4664\n",
            "Epoch 3 Batch 3850 Loss 1.1514 Accuracy 0.4668\n",
            "Epoch 3 Batch 3900 Loss 1.1497 Accuracy 0.4671\n",
            "Epoch 3 Batch 3950 Loss 1.1480 Accuracy 0.4675\n",
            "Epoch 3 Batch 4000 Loss 1.1458 Accuracy 0.4678\n",
            "Epoch 3 Batch 4050 Loss 1.1440 Accuracy 0.4682\n",
            "Epoch 3 Batch 4100 Loss 1.1430 Accuracy 0.4685\n",
            "Epoch 3 Batch 4150 Loss 1.1422 Accuracy 0.4688\n",
            "Epoch 3 Batch 4200 Loss 1.1420 Accuracy 0.4688\n",
            "Epoch 3 Batch 4250 Loss 1.1417 Accuracy 0.4689\n",
            "Epoch 3 Batch 4300 Loss 1.1421 Accuracy 0.4689\n",
            "Epoch 3 Batch 4350 Loss 1.1428 Accuracy 0.4688\n",
            "Epoch 3 Batch 4400 Loss 1.1436 Accuracy 0.4687\n",
            "Epoch 3 Batch 4450 Loss 1.1443 Accuracy 0.4686\n",
            "Epoch 3 Batch 4500 Loss 1.1454 Accuracy 0.4685\n",
            "Epoch 3 Batch 4550 Loss 1.1462 Accuracy 0.4683\n",
            "Epoch 3 Batch 4600 Loss 1.1470 Accuracy 0.4682\n",
            "Epoch 3 Batch 4650 Loss 1.1480 Accuracy 0.4682\n",
            "Epoch 3 Batch 4700 Loss 1.1492 Accuracy 0.4680\n",
            "Epoch 3 Batch 4750 Loss 1.1502 Accuracy 0.4678\n",
            "Epoch 3 Batch 4800 Loss 1.1510 Accuracy 0.4677\n",
            "Epoch 3 Batch 4850 Loss 1.1518 Accuracy 0.4676\n",
            "Epoch 3 Batch 4900 Loss 1.1530 Accuracy 0.4676\n",
            "Epoch 3 Batch 4950 Loss 1.1540 Accuracy 0.4674\n",
            "Epoch 3 Batch 5000 Loss 1.1548 Accuracy 0.4672\n",
            "Epoch 3 Batch 5050 Loss 1.1557 Accuracy 0.4671\n",
            "Epoch 3 Batch 5100 Loss 1.1566 Accuracy 0.4669\n",
            "Epoch 3 Batch 5150 Loss 1.1573 Accuracy 0.4668\n",
            "Epoch 3 Batch 5200 Loss 1.1581 Accuracy 0.4666\n",
            "Epoch 3 Batch 5250 Loss 1.1587 Accuracy 0.4665\n",
            "Epoch 3 Batch 5300 Loss 1.1592 Accuracy 0.4662\n",
            "Epoch 3 Batch 5350 Loss 1.1599 Accuracy 0.4661\n",
            "Epoch 3 Batch 5400 Loss 1.1607 Accuracy 0.4659\n",
            "Epoch 3 Batch 5450 Loss 1.1615 Accuracy 0.4656\n",
            "Epoch 3 Batch 5500 Loss 1.1621 Accuracy 0.4654\n",
            "Epoch 3 Batch 5550 Loss 1.1626 Accuracy 0.4653\n",
            "Epoch 3 Batch 5600 Loss 1.1633 Accuracy 0.4650\n",
            "Epoch 3 Batch 5650 Loss 1.1640 Accuracy 0.4648\n",
            "Epoch 3 Batch 5700 Loss 1.1640 Accuracy 0.4647\n",
            "Saving checkpoint for epoch 3 at ./drive/My Drive/projects/transformer/ckpt/ckpt-3\n",
            "Time taken for 1 epoch: 2040.486930847168 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.1238 Accuracy 0.4712\n",
            "Epoch 4 Batch 50 Loss 1.2356 Accuracy 0.4556\n",
            "Epoch 4 Batch 100 Loss 1.2318 Accuracy 0.4576\n",
            "Epoch 4 Batch 150 Loss 1.2278 Accuracy 0.4538\n",
            "Epoch 4 Batch 200 Loss 1.2282 Accuracy 0.4551\n",
            "Epoch 4 Batch 250 Loss 1.2215 Accuracy 0.4550\n",
            "Epoch 4 Batch 300 Loss 1.2182 Accuracy 0.4553\n",
            "Epoch 4 Batch 350 Loss 1.2184 Accuracy 0.4557\n",
            "Epoch 4 Batch 400 Loss 1.2126 Accuracy 0.4554\n",
            "Epoch 4 Batch 450 Loss 1.2095 Accuracy 0.4551\n",
            "Epoch 4 Batch 500 Loss 1.2063 Accuracy 0.4556\n",
            "Epoch 4 Batch 550 Loss 1.2038 Accuracy 0.4556\n",
            "Epoch 4 Batch 600 Loss 1.2004 Accuracy 0.4558\n",
            "Epoch 4 Batch 650 Loss 1.1977 Accuracy 0.4565\n",
            "Epoch 4 Batch 700 Loss 1.1963 Accuracy 0.4570\n",
            "Epoch 4 Batch 750 Loss 1.1968 Accuracy 0.4568\n",
            "Epoch 4 Batch 800 Loss 1.1977 Accuracy 0.4574\n",
            "Epoch 4 Batch 850 Loss 1.1959 Accuracy 0.4577\n",
            "Epoch 4 Batch 900 Loss 1.1950 Accuracy 0.4580\n",
            "Epoch 4 Batch 950 Loss 1.1941 Accuracy 0.4580\n",
            "Epoch 4 Batch 1000 Loss 1.1928 Accuracy 0.4582\n",
            "Epoch 4 Batch 1050 Loss 1.1910 Accuracy 0.4582\n",
            "Epoch 4 Batch 1100 Loss 1.1895 Accuracy 0.4587\n",
            "Epoch 4 Batch 1150 Loss 1.1871 Accuracy 0.4587\n",
            "Epoch 4 Batch 1200 Loss 1.1857 Accuracy 0.4589\n",
            "Epoch 4 Batch 1250 Loss 1.1839 Accuracy 0.4591\n",
            "Epoch 4 Batch 1300 Loss 1.1818 Accuracy 0.4595\n",
            "Epoch 4 Batch 1350 Loss 1.1800 Accuracy 0.4600\n",
            "Epoch 4 Batch 1400 Loss 1.1775 Accuracy 0.4608\n",
            "Epoch 4 Batch 1450 Loss 1.1744 Accuracy 0.4613\n",
            "Epoch 4 Batch 1500 Loss 1.1721 Accuracy 0.4621\n",
            "Epoch 4 Batch 1550 Loss 1.1690 Accuracy 0.4631\n",
            "Epoch 4 Batch 1600 Loss 1.1662 Accuracy 0.4641\n",
            "Epoch 4 Batch 1650 Loss 1.1635 Accuracy 0.4650\n",
            "Epoch 4 Batch 1700 Loss 1.1613 Accuracy 0.4658\n",
            "Epoch 4 Batch 1750 Loss 1.1590 Accuracy 0.4666\n",
            "Epoch 4 Batch 1800 Loss 1.1566 Accuracy 0.4673\n",
            "Epoch 4 Batch 1850 Loss 1.1543 Accuracy 0.4682\n",
            "Epoch 4 Batch 1900 Loss 1.1517 Accuracy 0.4688\n",
            "Epoch 4 Batch 1950 Loss 1.1499 Accuracy 0.4696\n",
            "Epoch 4 Batch 2000 Loss 1.1477 Accuracy 0.4701\n",
            "Epoch 4 Batch 2050 Loss 1.1452 Accuracy 0.4706\n",
            "Epoch 4 Batch 2100 Loss 1.1430 Accuracy 0.4710\n",
            "Epoch 4 Batch 2150 Loss 1.1397 Accuracy 0.4713\n",
            "Epoch 4 Batch 2200 Loss 1.1365 Accuracy 0.4715\n",
            "Epoch 4 Batch 2250 Loss 1.1325 Accuracy 0.4717\n",
            "Epoch 4 Batch 2300 Loss 1.1297 Accuracy 0.4719\n",
            "Epoch 4 Batch 2350 Loss 1.1269 Accuracy 0.4720\n",
            "Epoch 4 Batch 2400 Loss 1.1237 Accuracy 0.4724\n",
            "Epoch 4 Batch 2450 Loss 1.1208 Accuracy 0.4726\n",
            "Epoch 4 Batch 2500 Loss 1.1176 Accuracy 0.4729\n",
            "Epoch 4 Batch 2550 Loss 1.1146 Accuracy 0.4733\n",
            "Epoch 4 Batch 2600 Loss 1.1120 Accuracy 0.4737\n",
            "Epoch 4 Batch 2650 Loss 1.1089 Accuracy 0.4741\n",
            "Epoch 4 Batch 2700 Loss 1.1067 Accuracy 0.4744\n",
            "Epoch 4 Batch 2750 Loss 1.1035 Accuracy 0.4747\n",
            "Epoch 4 Batch 2800 Loss 1.1012 Accuracy 0.4750\n",
            "Epoch 4 Batch 2850 Loss 1.0992 Accuracy 0.4754\n",
            "Epoch 4 Batch 2900 Loss 1.0968 Accuracy 0.4757\n",
            "Epoch 4 Batch 2950 Loss 1.0943 Accuracy 0.4760\n",
            "Epoch 4 Batch 3000 Loss 1.0920 Accuracy 0.4763\n",
            "Epoch 4 Batch 3050 Loss 1.0897 Accuracy 0.4767\n",
            "Epoch 4 Batch 3100 Loss 1.0876 Accuracy 0.4770\n",
            "Epoch 4 Batch 3150 Loss 1.0855 Accuracy 0.4772\n",
            "Epoch 4 Batch 3200 Loss 1.0835 Accuracy 0.4774\n",
            "Epoch 4 Batch 3250 Loss 1.0817 Accuracy 0.4778\n",
            "Epoch 4 Batch 3300 Loss 1.0793 Accuracy 0.4781\n",
            "Epoch 4 Batch 3350 Loss 1.0768 Accuracy 0.4784\n",
            "Epoch 4 Batch 3400 Loss 1.0747 Accuracy 0.4787\n",
            "Epoch 4 Batch 3450 Loss 1.0725 Accuracy 0.4790\n",
            "Epoch 4 Batch 3500 Loss 1.0705 Accuracy 0.4794\n",
            "Epoch 4 Batch 3550 Loss 1.0688 Accuracy 0.4799\n",
            "Epoch 4 Batch 3600 Loss 1.0668 Accuracy 0.4803\n",
            "Epoch 4 Batch 3650 Loss 1.0645 Accuracy 0.4806\n",
            "Epoch 4 Batch 3700 Loss 1.0626 Accuracy 0.4809\n",
            "Epoch 4 Batch 3750 Loss 1.0610 Accuracy 0.4812\n",
            "Epoch 4 Batch 3800 Loss 1.0591 Accuracy 0.4815\n",
            "Epoch 4 Batch 3850 Loss 1.0577 Accuracy 0.4819\n",
            "Epoch 4 Batch 3900 Loss 1.0558 Accuracy 0.4823\n",
            "Epoch 4 Batch 3950 Loss 1.0545 Accuracy 0.4825\n",
            "Epoch 4 Batch 4000 Loss 1.0529 Accuracy 0.4828\n",
            "Epoch 4 Batch 4050 Loss 1.0514 Accuracy 0.4831\n",
            "Epoch 4 Batch 4100 Loss 1.0500 Accuracy 0.4833\n",
            "Epoch 4 Batch 4150 Loss 1.0495 Accuracy 0.4834\n",
            "Epoch 4 Batch 4200 Loss 1.0496 Accuracy 0.4835\n",
            "Epoch 4 Batch 4250 Loss 1.0502 Accuracy 0.4835\n",
            "Epoch 4 Batch 4300 Loss 1.0510 Accuracy 0.4834\n",
            "Epoch 4 Batch 4350 Loss 1.0514 Accuracy 0.4834\n",
            "Epoch 4 Batch 4400 Loss 1.0521 Accuracy 0.4833\n",
            "Epoch 4 Batch 4450 Loss 1.0532 Accuracy 0.4832\n",
            "Epoch 4 Batch 4500 Loss 1.0543 Accuracy 0.4830\n",
            "Epoch 4 Batch 4550 Loss 1.0554 Accuracy 0.4828\n",
            "Epoch 4 Batch 4600 Loss 1.0565 Accuracy 0.4826\n",
            "Epoch 4 Batch 4650 Loss 1.0579 Accuracy 0.4825\n",
            "Epoch 4 Batch 4700 Loss 1.0594 Accuracy 0.4823\n",
            "Epoch 4 Batch 4750 Loss 1.0606 Accuracy 0.4821\n",
            "Epoch 4 Batch 4800 Loss 1.0616 Accuracy 0.4820\n",
            "Epoch 4 Batch 4850 Loss 1.0627 Accuracy 0.4818\n",
            "Epoch 4 Batch 4900 Loss 1.0638 Accuracy 0.4817\n",
            "Epoch 4 Batch 4950 Loss 1.0649 Accuracy 0.4816\n",
            "Epoch 4 Batch 5000 Loss 1.0658 Accuracy 0.4814\n",
            "Epoch 4 Batch 5050 Loss 1.0670 Accuracy 0.4812\n",
            "Epoch 4 Batch 5100 Loss 1.0680 Accuracy 0.4810\n",
            "Epoch 4 Batch 5150 Loss 1.0692 Accuracy 0.4808\n",
            "Epoch 4 Batch 5200 Loss 1.0707 Accuracy 0.4806\n",
            "Epoch 4 Batch 5250 Loss 1.0717 Accuracy 0.4803\n",
            "Epoch 4 Batch 5300 Loss 1.0726 Accuracy 0.4800\n",
            "Epoch 4 Batch 5350 Loss 1.0736 Accuracy 0.4798\n",
            "Epoch 4 Batch 5400 Loss 1.0743 Accuracy 0.4796\n",
            "Epoch 4 Batch 5450 Loss 1.0752 Accuracy 0.4793\n",
            "Epoch 4 Batch 5500 Loss 1.0761 Accuracy 0.4791\n",
            "Epoch 4 Batch 5550 Loss 1.0767 Accuracy 0.4789\n",
            "Epoch 4 Batch 5600 Loss 1.0774 Accuracy 0.4787\n",
            "Epoch 4 Batch 5650 Loss 1.0782 Accuracy 0.4785\n",
            "Epoch 4 Batch 5700 Loss 1.0789 Accuracy 0.4783\n",
            "Saving checkpoint for epoch 4 at ./drive/My Drive/projects/transformer/ckpt/ckpt-4\n",
            "Time taken for 1 epoch: 2028.6619956493378 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.1304 Accuracy 0.4630\n",
            "Epoch 5 Batch 50 Loss 1.1724 Accuracy 0.4619\n",
            "Epoch 5 Batch 100 Loss 1.1615 Accuracy 0.4618\n",
            "Epoch 5 Batch 150 Loss 1.1571 Accuracy 0.4638\n",
            "Epoch 5 Batch 200 Loss 1.1551 Accuracy 0.4659\n",
            "Epoch 5 Batch 250 Loss 1.1532 Accuracy 0.4661\n",
            "Epoch 5 Batch 300 Loss 1.1529 Accuracy 0.4658\n",
            "Epoch 5 Batch 350 Loss 1.1518 Accuracy 0.4655\n",
            "Epoch 5 Batch 400 Loss 1.1515 Accuracy 0.4662\n",
            "Epoch 5 Batch 450 Loss 1.1492 Accuracy 0.4659\n",
            "Epoch 5 Batch 500 Loss 1.1465 Accuracy 0.4657\n",
            "Epoch 5 Batch 550 Loss 1.1442 Accuracy 0.4656\n",
            "Epoch 5 Batch 600 Loss 1.1423 Accuracy 0.4656\n",
            "Epoch 5 Batch 650 Loss 1.1426 Accuracy 0.4658\n",
            "Epoch 5 Batch 700 Loss 1.1423 Accuracy 0.4663\n",
            "Epoch 5 Batch 750 Loss 1.1410 Accuracy 0.4669\n",
            "Epoch 5 Batch 800 Loss 1.1382 Accuracy 0.4674\n",
            "Epoch 5 Batch 850 Loss 1.1377 Accuracy 0.4673\n",
            "Epoch 5 Batch 900 Loss 1.1371 Accuracy 0.4672\n",
            "Epoch 5 Batch 950 Loss 1.1352 Accuracy 0.4670\n",
            "Epoch 5 Batch 1000 Loss 1.1328 Accuracy 0.4673\n",
            "Epoch 5 Batch 1050 Loss 1.1309 Accuracy 0.4675\n",
            "Epoch 5 Batch 1100 Loss 1.1293 Accuracy 0.4675\n",
            "Epoch 5 Batch 1150 Loss 1.1283 Accuracy 0.4678\n",
            "Epoch 5 Batch 1200 Loss 1.1272 Accuracy 0.4680\n",
            "Epoch 5 Batch 1250 Loss 1.1252 Accuracy 0.4684\n",
            "Epoch 5 Batch 1300 Loss 1.1233 Accuracy 0.4690\n",
            "Epoch 5 Batch 1350 Loss 1.1201 Accuracy 0.4696\n",
            "Epoch 5 Batch 1400 Loss 1.1175 Accuracy 0.4703\n",
            "Epoch 5 Batch 1450 Loss 1.1153 Accuracy 0.4711\n",
            "Epoch 5 Batch 1500 Loss 1.1130 Accuracy 0.4718\n",
            "Epoch 5 Batch 1550 Loss 1.1105 Accuracy 0.4726\n",
            "Epoch 5 Batch 1600 Loss 1.1078 Accuracy 0.4736\n",
            "Epoch 5 Batch 1650 Loss 1.1057 Accuracy 0.4744\n",
            "Epoch 5 Batch 1700 Loss 1.1029 Accuracy 0.4752\n",
            "Epoch 5 Batch 1750 Loss 1.1002 Accuracy 0.4759\n",
            "Epoch 5 Batch 1800 Loss 1.0979 Accuracy 0.4767\n",
            "Epoch 5 Batch 1850 Loss 1.0956 Accuracy 0.4774\n",
            "Epoch 5 Batch 1900 Loss 1.0933 Accuracy 0.4784\n",
            "Epoch 5 Batch 1950 Loss 1.0908 Accuracy 0.4791\n",
            "Epoch 5 Batch 2000 Loss 1.0890 Accuracy 0.4796\n",
            "Epoch 5 Batch 2050 Loss 1.0867 Accuracy 0.4800\n",
            "Epoch 5 Batch 2100 Loss 1.0846 Accuracy 0.4804\n",
            "Epoch 5 Batch 2150 Loss 1.0816 Accuracy 0.4806\n",
            "Epoch 5 Batch 2200 Loss 1.0786 Accuracy 0.4808\n",
            "Epoch 5 Batch 2250 Loss 1.0750 Accuracy 0.4808\n",
            "Epoch 5 Batch 2300 Loss 1.0725 Accuracy 0.4811\n",
            "Epoch 5 Batch 2350 Loss 1.0696 Accuracy 0.4814\n",
            "Epoch 5 Batch 2400 Loss 1.0667 Accuracy 0.4817\n",
            "Epoch 5 Batch 2450 Loss 1.0635 Accuracy 0.4820\n",
            "Epoch 5 Batch 2500 Loss 1.0609 Accuracy 0.4823\n",
            "Epoch 5 Batch 2550 Loss 1.0579 Accuracy 0.4826\n",
            "Epoch 5 Batch 2600 Loss 1.0549 Accuracy 0.4829\n",
            "Epoch 5 Batch 2650 Loss 1.0523 Accuracy 0.4832\n",
            "Epoch 5 Batch 2700 Loss 1.0493 Accuracy 0.4836\n",
            "Epoch 5 Batch 2750 Loss 1.0469 Accuracy 0.4839\n",
            "Epoch 5 Batch 2800 Loss 1.0447 Accuracy 0.4843\n",
            "Epoch 5 Batch 2850 Loss 1.0424 Accuracy 0.4845\n",
            "Epoch 5 Batch 2900 Loss 1.0403 Accuracy 0.4848\n",
            "Epoch 5 Batch 2950 Loss 1.0384 Accuracy 0.4851\n",
            "Epoch 5 Batch 3000 Loss 1.0364 Accuracy 0.4855\n",
            "Epoch 5 Batch 3050 Loss 1.0340 Accuracy 0.4858\n",
            "Epoch 5 Batch 3100 Loss 1.0320 Accuracy 0.4861\n",
            "Epoch 5 Batch 3150 Loss 1.0300 Accuracy 0.4864\n",
            "Epoch 5 Batch 3200 Loss 1.0278 Accuracy 0.4866\n",
            "Epoch 5 Batch 3250 Loss 1.0258 Accuracy 0.4870\n",
            "Epoch 5 Batch 3300 Loss 1.0234 Accuracy 0.4872\n",
            "Epoch 5 Batch 3350 Loss 1.0212 Accuracy 0.4875\n",
            "Epoch 5 Batch 3400 Loss 1.0188 Accuracy 0.4878\n",
            "Epoch 5 Batch 3450 Loss 1.0172 Accuracy 0.4881\n",
            "Epoch 5 Batch 3500 Loss 1.0155 Accuracy 0.4884\n",
            "Epoch 5 Batch 3550 Loss 1.0133 Accuracy 0.4888\n",
            "Epoch 5 Batch 3600 Loss 1.0114 Accuracy 0.4892\n",
            "Epoch 5 Batch 3650 Loss 1.0100 Accuracy 0.4895\n",
            "Epoch 5 Batch 3700 Loss 1.0082 Accuracy 0.4897\n",
            "Epoch 5 Batch 3750 Loss 1.0068 Accuracy 0.4901\n",
            "Epoch 5 Batch 3800 Loss 1.0051 Accuracy 0.4903\n",
            "Epoch 5 Batch 3850 Loss 1.0038 Accuracy 0.4907\n",
            "Epoch 5 Batch 3900 Loss 1.0023 Accuracy 0.4910\n",
            "Epoch 5 Batch 3950 Loss 1.0011 Accuracy 0.4913\n",
            "Epoch 5 Batch 4000 Loss 0.9994 Accuracy 0.4917\n",
            "Epoch 5 Batch 4050 Loss 0.9977 Accuracy 0.4919\n",
            "Epoch 5 Batch 4100 Loss 0.9966 Accuracy 0.4921\n",
            "Epoch 5 Batch 4150 Loss 0.9959 Accuracy 0.4922\n",
            "Epoch 5 Batch 4200 Loss 0.9959 Accuracy 0.4923\n",
            "Epoch 5 Batch 4250 Loss 0.9964 Accuracy 0.4922\n",
            "Epoch 5 Batch 4300 Loss 0.9970 Accuracy 0.4922\n",
            "Epoch 5 Batch 4350 Loss 0.9979 Accuracy 0.4921\n",
            "Epoch 5 Batch 4400 Loss 0.9989 Accuracy 0.4920\n",
            "Epoch 5 Batch 4450 Loss 1.0000 Accuracy 0.4918\n",
            "Epoch 5 Batch 4500 Loss 1.0012 Accuracy 0.4917\n",
            "Epoch 5 Batch 4550 Loss 1.0026 Accuracy 0.4915\n",
            "Epoch 5 Batch 4600 Loss 1.0042 Accuracy 0.4913\n",
            "Epoch 5 Batch 4650 Loss 1.0054 Accuracy 0.4911\n",
            "Epoch 5 Batch 4700 Loss 1.0068 Accuracy 0.4909\n",
            "Epoch 5 Batch 4750 Loss 1.0081 Accuracy 0.4908\n",
            "Epoch 5 Batch 4800 Loss 1.0092 Accuracy 0.4907\n",
            "Epoch 5 Batch 4850 Loss 1.0103 Accuracy 0.4905\n",
            "Epoch 5 Batch 4900 Loss 1.0115 Accuracy 0.4903\n",
            "Epoch 5 Batch 4950 Loss 1.0128 Accuracy 0.4901\n",
            "Epoch 5 Batch 5000 Loss 1.0136 Accuracy 0.4899\n",
            "Epoch 5 Batch 5050 Loss 1.0148 Accuracy 0.4897\n",
            "Epoch 5 Batch 5100 Loss 1.0162 Accuracy 0.4895\n",
            "Epoch 5 Batch 5150 Loss 1.0175 Accuracy 0.4893\n",
            "Epoch 5 Batch 5200 Loss 1.0189 Accuracy 0.4891\n",
            "Epoch 5 Batch 5250 Loss 1.0200 Accuracy 0.4889\n",
            "Epoch 5 Batch 5300 Loss 1.0208 Accuracy 0.4886\n",
            "Epoch 5 Batch 5350 Loss 1.0220 Accuracy 0.4883\n",
            "Epoch 5 Batch 5400 Loss 1.0230 Accuracy 0.4881\n",
            "Epoch 5 Batch 5450 Loss 1.0239 Accuracy 0.4878\n",
            "Epoch 5 Batch 5500 Loss 1.0247 Accuracy 0.4876\n",
            "Epoch 5 Batch 5550 Loss 1.0255 Accuracy 0.4873\n",
            "Epoch 5 Batch 5600 Loss 1.0265 Accuracy 0.4871\n",
            "Epoch 5 Batch 5650 Loss 1.0274 Accuracy 0.4869\n",
            "Epoch 5 Batch 5700 Loss 1.0281 Accuracy 0.4867\n",
            "Saving checkpoint for epoch 5 at ./drive/My Drive/projects/transformer/ckpt/ckpt-5\n",
            "Time taken for 1 epoch: 2026.2641115188599 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.1764 Accuracy 0.4794\n",
            "Epoch 6 Batch 50 Loss 1.1139 Accuracy 0.4700\n",
            "Epoch 6 Batch 100 Loss 1.1090 Accuracy 0.4721\n",
            "Epoch 6 Batch 150 Loss 1.1175 Accuracy 0.4724\n",
            "Epoch 6 Batch 200 Loss 1.1153 Accuracy 0.4722\n",
            "Epoch 6 Batch 250 Loss 1.1092 Accuracy 0.4729\n",
            "Epoch 6 Batch 300 Loss 1.1093 Accuracy 0.4727\n",
            "Epoch 6 Batch 350 Loss 1.1091 Accuracy 0.4729\n",
            "Epoch 6 Batch 400 Loss 1.1048 Accuracy 0.4730\n",
            "Epoch 6 Batch 450 Loss 1.1035 Accuracy 0.4723\n",
            "Epoch 6 Batch 500 Loss 1.1022 Accuracy 0.4721\n",
            "Epoch 6 Batch 550 Loss 1.0991 Accuracy 0.4724\n",
            "Epoch 6 Batch 600 Loss 1.0994 Accuracy 0.4724\n",
            "Epoch 6 Batch 650 Loss 1.0994 Accuracy 0.4728\n",
            "Epoch 6 Batch 700 Loss 1.0972 Accuracy 0.4730\n",
            "Epoch 6 Batch 750 Loss 1.0968 Accuracy 0.4731\n",
            "Epoch 6 Batch 800 Loss 1.0972 Accuracy 0.4734\n",
            "Epoch 6 Batch 850 Loss 1.0960 Accuracy 0.4738\n",
            "Epoch 6 Batch 900 Loss 1.0952 Accuracy 0.4736\n",
            "Epoch 6 Batch 950 Loss 1.0930 Accuracy 0.4736\n",
            "Epoch 6 Batch 1000 Loss 1.0907 Accuracy 0.4735\n",
            "Epoch 6 Batch 1050 Loss 1.0892 Accuracy 0.4738\n",
            "Epoch 6 Batch 1100 Loss 1.0885 Accuracy 0.4742\n",
            "Epoch 6 Batch 1150 Loss 1.0876 Accuracy 0.4742\n",
            "Epoch 6 Batch 1200 Loss 1.0865 Accuracy 0.4743\n",
            "Epoch 6 Batch 1250 Loss 1.0846 Accuracy 0.4747\n",
            "Epoch 6 Batch 1300 Loss 1.0825 Accuracy 0.4750\n",
            "Epoch 6 Batch 1350 Loss 1.0803 Accuracy 0.4756\n",
            "Epoch 6 Batch 1400 Loss 1.0781 Accuracy 0.4763\n",
            "Epoch 6 Batch 1450 Loss 1.0756 Accuracy 0.4771\n",
            "Epoch 6 Batch 1500 Loss 1.0727 Accuracy 0.4780\n",
            "Epoch 6 Batch 1550 Loss 1.0704 Accuracy 0.4788\n",
            "Epoch 6 Batch 1600 Loss 1.0675 Accuracy 0.4799\n",
            "Epoch 6 Batch 1650 Loss 1.0651 Accuracy 0.4807\n",
            "Epoch 6 Batch 1700 Loss 1.0626 Accuracy 0.4815\n",
            "Epoch 6 Batch 1750 Loss 1.0603 Accuracy 0.4823\n",
            "Epoch 6 Batch 1800 Loss 1.0580 Accuracy 0.4829\n",
            "Epoch 6 Batch 1850 Loss 1.0558 Accuracy 0.4837\n",
            "Epoch 6 Batch 1900 Loss 1.0537 Accuracy 0.4846\n",
            "Epoch 6 Batch 1950 Loss 1.0515 Accuracy 0.4853\n",
            "Epoch 6 Batch 2000 Loss 1.0494 Accuracy 0.4860\n",
            "Epoch 6 Batch 2050 Loss 1.0475 Accuracy 0.4864\n",
            "Epoch 6 Batch 2100 Loss 1.0452 Accuracy 0.4867\n",
            "Epoch 6 Batch 2150 Loss 1.0424 Accuracy 0.4869\n",
            "Epoch 6 Batch 2200 Loss 1.0399 Accuracy 0.4870\n",
            "Epoch 6 Batch 2250 Loss 1.0371 Accuracy 0.4871\n",
            "Epoch 6 Batch 2300 Loss 1.0341 Accuracy 0.4874\n",
            "Epoch 6 Batch 2350 Loss 1.0311 Accuracy 0.4876\n",
            "Epoch 6 Batch 2400 Loss 1.0284 Accuracy 0.4879\n",
            "Epoch 6 Batch 2450 Loss 1.0254 Accuracy 0.4881\n",
            "Epoch 6 Batch 2500 Loss 1.0218 Accuracy 0.4884\n",
            "Epoch 6 Batch 2550 Loss 1.0187 Accuracy 0.4886\n",
            "Epoch 6 Batch 2600 Loss 1.0158 Accuracy 0.4890\n",
            "Epoch 6 Batch 2650 Loss 1.0134 Accuracy 0.4893\n",
            "Epoch 6 Batch 2700 Loss 1.0110 Accuracy 0.4896\n",
            "Epoch 6 Batch 2750 Loss 1.0089 Accuracy 0.4898\n",
            "Epoch 6 Batch 2800 Loss 1.0067 Accuracy 0.4902\n",
            "Epoch 6 Batch 2850 Loss 1.0044 Accuracy 0.4905\n",
            "Epoch 6 Batch 2900 Loss 1.0027 Accuracy 0.4909\n",
            "Epoch 6 Batch 2950 Loss 1.0006 Accuracy 0.4912\n",
            "Epoch 6 Batch 3000 Loss 0.9986 Accuracy 0.4915\n",
            "Epoch 6 Batch 3050 Loss 0.9966 Accuracy 0.4917\n",
            "Epoch 6 Batch 3100 Loss 0.9949 Accuracy 0.4921\n",
            "Epoch 6 Batch 3150 Loss 0.9932 Accuracy 0.4923\n",
            "Epoch 6 Batch 3200 Loss 0.9910 Accuracy 0.4926\n",
            "Epoch 6 Batch 3250 Loss 0.9889 Accuracy 0.4929\n",
            "Epoch 6 Batch 3300 Loss 0.9869 Accuracy 0.4932\n",
            "Epoch 6 Batch 3350 Loss 0.9849 Accuracy 0.4934\n",
            "Epoch 6 Batch 3400 Loss 0.9826 Accuracy 0.4937\n",
            "Epoch 6 Batch 3450 Loss 0.9804 Accuracy 0.4940\n",
            "Epoch 6 Batch 3500 Loss 0.9785 Accuracy 0.4942\n",
            "Epoch 6 Batch 3550 Loss 0.9768 Accuracy 0.4945\n",
            "Epoch 6 Batch 3600 Loss 0.9748 Accuracy 0.4948\n",
            "Epoch 6 Batch 3650 Loss 0.9729 Accuracy 0.4951\n",
            "Epoch 6 Batch 3700 Loss 0.9714 Accuracy 0.4954\n",
            "Epoch 6 Batch 3750 Loss 0.9699 Accuracy 0.4957\n",
            "Epoch 6 Batch 3800 Loss 0.9683 Accuracy 0.4961\n",
            "Epoch 6 Batch 3850 Loss 0.9672 Accuracy 0.4964\n",
            "Epoch 6 Batch 3900 Loss 0.9659 Accuracy 0.4967\n",
            "Epoch 6 Batch 3950 Loss 0.9643 Accuracy 0.4971\n",
            "Epoch 6 Batch 4000 Loss 0.9629 Accuracy 0.4974\n",
            "Epoch 6 Batch 4050 Loss 0.9612 Accuracy 0.4977\n",
            "Epoch 6 Batch 4100 Loss 0.9601 Accuracy 0.4980\n",
            "Epoch 6 Batch 4150 Loss 0.9596 Accuracy 0.4982\n",
            "Epoch 6 Batch 4200 Loss 0.9597 Accuracy 0.4982\n",
            "Epoch 6 Batch 4250 Loss 0.9603 Accuracy 0.4981\n",
            "Epoch 6 Batch 4300 Loss 0.9611 Accuracy 0.4981\n",
            "Epoch 6 Batch 4350 Loss 0.9618 Accuracy 0.4980\n",
            "Epoch 6 Batch 4400 Loss 0.9626 Accuracy 0.4978\n",
            "Epoch 6 Batch 4450 Loss 0.9637 Accuracy 0.4976\n",
            "Epoch 6 Batch 4500 Loss 0.9649 Accuracy 0.4974\n",
            "Epoch 6 Batch 4550 Loss 0.9664 Accuracy 0.4973\n",
            "Epoch 6 Batch 4600 Loss 0.9679 Accuracy 0.4972\n",
            "Epoch 6 Batch 4650 Loss 0.9693 Accuracy 0.4970\n",
            "Epoch 6 Batch 4700 Loss 0.9707 Accuracy 0.4968\n",
            "Epoch 6 Batch 4750 Loss 0.9719 Accuracy 0.4966\n",
            "Epoch 6 Batch 4800 Loss 0.9731 Accuracy 0.4964\n",
            "Epoch 6 Batch 4850 Loss 0.9743 Accuracy 0.4962\n",
            "Epoch 6 Batch 4900 Loss 0.9755 Accuracy 0.4961\n",
            "Epoch 6 Batch 4950 Loss 0.9767 Accuracy 0.4959\n",
            "Epoch 6 Batch 5000 Loss 0.9781 Accuracy 0.4957\n",
            "Epoch 6 Batch 5050 Loss 0.9794 Accuracy 0.4955\n",
            "Epoch 6 Batch 5100 Loss 0.9808 Accuracy 0.4952\n",
            "Epoch 6 Batch 5150 Loss 0.9821 Accuracy 0.4950\n",
            "Epoch 6 Batch 5200 Loss 0.9833 Accuracy 0.4947\n",
            "Epoch 6 Batch 5250 Loss 0.9844 Accuracy 0.4945\n",
            "Epoch 6 Batch 5300 Loss 0.9855 Accuracy 0.4942\n",
            "Epoch 6 Batch 5350 Loss 0.9866 Accuracy 0.4939\n",
            "Epoch 6 Batch 5400 Loss 0.9877 Accuracy 0.4936\n",
            "Epoch 6 Batch 5450 Loss 0.9885 Accuracy 0.4933\n",
            "Epoch 6 Batch 5500 Loss 0.9893 Accuracy 0.4931\n",
            "Epoch 6 Batch 5550 Loss 0.9901 Accuracy 0.4928\n",
            "Epoch 6 Batch 5600 Loss 0.9911 Accuracy 0.4926\n",
            "Epoch 6 Batch 5650 Loss 0.9917 Accuracy 0.4924\n",
            "Epoch 6 Batch 5700 Loss 0.9927 Accuracy 0.4922\n",
            "Saving checkpoint for epoch 6 at ./drive/My Drive/projects/transformer/ckpt/ckpt-6\n",
            "Time taken for 1 epoch: 2026.5055050849915 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 1.1051 Accuracy 0.4975\n",
            "Epoch 7 Batch 50 Loss 1.0728 Accuracy 0.4713\n",
            "Epoch 7 Batch 100 Loss 1.0723 Accuracy 0.4753\n",
            "Epoch 7 Batch 150 Loss 1.0786 Accuracy 0.4759\n",
            "Epoch 7 Batch 200 Loss 1.0839 Accuracy 0.4768\n",
            "Epoch 7 Batch 250 Loss 1.0813 Accuracy 0.4780\n",
            "Epoch 7 Batch 300 Loss 1.0851 Accuracy 0.4774\n",
            "Epoch 7 Batch 350 Loss 1.0824 Accuracy 0.4777\n",
            "Epoch 7 Batch 400 Loss 1.0813 Accuracy 0.4771\n",
            "Epoch 7 Batch 450 Loss 1.0789 Accuracy 0.4771\n",
            "Epoch 7 Batch 500 Loss 1.0760 Accuracy 0.4770\n",
            "Epoch 7 Batch 550 Loss 1.0729 Accuracy 0.4771\n",
            "Epoch 7 Batch 600 Loss 1.0714 Accuracy 0.4772\n",
            "Epoch 7 Batch 650 Loss 1.0712 Accuracy 0.4775\n",
            "Epoch 7 Batch 700 Loss 1.0709 Accuracy 0.4778\n",
            "Epoch 7 Batch 750 Loss 1.0708 Accuracy 0.4783\n",
            "Epoch 7 Batch 800 Loss 1.0701 Accuracy 0.4786\n",
            "Epoch 7 Batch 850 Loss 1.0677 Accuracy 0.4786\n",
            "Epoch 7 Batch 900 Loss 1.0675 Accuracy 0.4787\n",
            "Epoch 7 Batch 950 Loss 1.0667 Accuracy 0.4784\n",
            "Epoch 7 Batch 1000 Loss 1.0643 Accuracy 0.4786\n",
            "Epoch 7 Batch 1050 Loss 1.0627 Accuracy 0.4788\n",
            "Epoch 7 Batch 1100 Loss 1.0616 Accuracy 0.4789\n",
            "Epoch 7 Batch 1150 Loss 1.0600 Accuracy 0.4791\n",
            "Epoch 7 Batch 1200 Loss 1.0585 Accuracy 0.4791\n",
            "Epoch 7 Batch 1250 Loss 1.0563 Accuracy 0.4795\n",
            "Epoch 7 Batch 1300 Loss 1.0553 Accuracy 0.4799\n",
            "Epoch 7 Batch 1350 Loss 1.0524 Accuracy 0.4807\n",
            "Epoch 7 Batch 1400 Loss 1.0494 Accuracy 0.4813\n",
            "Epoch 7 Batch 1450 Loss 1.0475 Accuracy 0.4820\n",
            "Epoch 7 Batch 1500 Loss 1.0450 Accuracy 0.4829\n",
            "Epoch 7 Batch 1550 Loss 1.0425 Accuracy 0.4838\n",
            "Epoch 7 Batch 1600 Loss 1.0398 Accuracy 0.4844\n",
            "Epoch 7 Batch 1650 Loss 1.0378 Accuracy 0.4851\n",
            "Epoch 7 Batch 1700 Loss 1.0353 Accuracy 0.4859\n",
            "Epoch 7 Batch 1750 Loss 1.0324 Accuracy 0.4866\n",
            "Epoch 7 Batch 1800 Loss 1.0302 Accuracy 0.4874\n",
            "Epoch 7 Batch 1850 Loss 1.0281 Accuracy 0.4882\n",
            "Epoch 7 Batch 1900 Loss 1.0255 Accuracy 0.4890\n",
            "Epoch 7 Batch 1950 Loss 1.0231 Accuracy 0.4896\n",
            "Epoch 7 Batch 2000 Loss 1.0211 Accuracy 0.4903\n",
            "Epoch 7 Batch 2050 Loss 1.0187 Accuracy 0.4908\n",
            "Epoch 7 Batch 2100 Loss 1.0157 Accuracy 0.4912\n",
            "Epoch 7 Batch 2150 Loss 1.0129 Accuracy 0.4915\n",
            "Epoch 7 Batch 2200 Loss 1.0097 Accuracy 0.4917\n",
            "Epoch 7 Batch 2250 Loss 1.0069 Accuracy 0.4920\n",
            "Epoch 7 Batch 2300 Loss 1.0042 Accuracy 0.4921\n",
            "Epoch 7 Batch 2350 Loss 1.0017 Accuracy 0.4923\n",
            "Epoch 7 Batch 2400 Loss 0.9986 Accuracy 0.4926\n",
            "Epoch 7 Batch 2450 Loss 0.9961 Accuracy 0.4930\n",
            "Epoch 7 Batch 2500 Loss 0.9935 Accuracy 0.4932\n",
            "Epoch 7 Batch 2550 Loss 0.9906 Accuracy 0.4935\n",
            "Epoch 7 Batch 2600 Loss 0.9880 Accuracy 0.4939\n",
            "Epoch 7 Batch 2650 Loss 0.9848 Accuracy 0.4943\n",
            "Epoch 7 Batch 2700 Loss 0.9824 Accuracy 0.4945\n",
            "Epoch 7 Batch 2750 Loss 0.9805 Accuracy 0.4948\n",
            "Epoch 7 Batch 2800 Loss 0.9781 Accuracy 0.4952\n",
            "Epoch 7 Batch 2850 Loss 0.9757 Accuracy 0.4956\n",
            "Epoch 7 Batch 2900 Loss 0.9733 Accuracy 0.4959\n",
            "Epoch 7 Batch 2950 Loss 0.9713 Accuracy 0.4961\n",
            "Epoch 7 Batch 3000 Loss 0.9693 Accuracy 0.4963\n",
            "Epoch 7 Batch 3050 Loss 0.9677 Accuracy 0.4965\n",
            "Epoch 7 Batch 3100 Loss 0.9657 Accuracy 0.4969\n",
            "Epoch 7 Batch 3150 Loss 0.9639 Accuracy 0.4971\n",
            "Epoch 7 Batch 3200 Loss 0.9620 Accuracy 0.4973\n",
            "Epoch 7 Batch 3250 Loss 0.9600 Accuracy 0.4975\n",
            "Epoch 7 Batch 3300 Loss 0.9580 Accuracy 0.4978\n",
            "Epoch 7 Batch 3350 Loss 0.9563 Accuracy 0.4981\n",
            "Epoch 7 Batch 3400 Loss 0.9544 Accuracy 0.4984\n",
            "Epoch 7 Batch 3450 Loss 0.9522 Accuracy 0.4987\n",
            "Epoch 7 Batch 3500 Loss 0.9505 Accuracy 0.4990\n",
            "Epoch 7 Batch 3550 Loss 0.9486 Accuracy 0.4992\n",
            "Epoch 7 Batch 3600 Loss 0.9468 Accuracy 0.4995\n",
            "Epoch 7 Batch 3650 Loss 0.9453 Accuracy 0.4998\n",
            "Epoch 7 Batch 3700 Loss 0.9435 Accuracy 0.5002\n",
            "Epoch 7 Batch 3750 Loss 0.9423 Accuracy 0.5005\n",
            "Epoch 7 Batch 3800 Loss 0.9409 Accuracy 0.5008\n",
            "Epoch 7 Batch 3850 Loss 0.9393 Accuracy 0.5011\n",
            "Epoch 7 Batch 3900 Loss 0.9380 Accuracy 0.5015\n",
            "Epoch 7 Batch 3950 Loss 0.9366 Accuracy 0.5018\n",
            "Epoch 7 Batch 4000 Loss 0.9353 Accuracy 0.5021\n",
            "Epoch 7 Batch 4050 Loss 0.9341 Accuracy 0.5025\n",
            "Epoch 7 Batch 4100 Loss 0.9330 Accuracy 0.5028\n",
            "Epoch 7 Batch 4150 Loss 0.9328 Accuracy 0.5029\n",
            "Epoch 7 Batch 4200 Loss 0.9328 Accuracy 0.5030\n",
            "Epoch 7 Batch 4250 Loss 0.9330 Accuracy 0.5030\n",
            "Epoch 7 Batch 4300 Loss 0.9336 Accuracy 0.5028\n",
            "Epoch 7 Batch 4350 Loss 0.9347 Accuracy 0.5027\n",
            "Epoch 7 Batch 4400 Loss 0.9358 Accuracy 0.5025\n",
            "Epoch 7 Batch 4450 Loss 0.9367 Accuracy 0.5023\n",
            "Epoch 7 Batch 4500 Loss 0.9378 Accuracy 0.5021\n",
            "Epoch 7 Batch 4550 Loss 0.9392 Accuracy 0.5019\n",
            "Epoch 7 Batch 4600 Loss 0.9405 Accuracy 0.5017\n",
            "Epoch 7 Batch 4650 Loss 0.9419 Accuracy 0.5015\n",
            "Epoch 7 Batch 4700 Loss 0.9433 Accuracy 0.5013\n",
            "Epoch 7 Batch 4750 Loss 0.9445 Accuracy 0.5011\n",
            "Epoch 7 Batch 4800 Loss 0.9458 Accuracy 0.5009\n",
            "Epoch 7 Batch 4850 Loss 0.9473 Accuracy 0.5007\n",
            "Epoch 7 Batch 4900 Loss 0.9486 Accuracy 0.5005\n",
            "Epoch 7 Batch 4950 Loss 0.9498 Accuracy 0.5003\n",
            "Epoch 7 Batch 5000 Loss 0.9510 Accuracy 0.5001\n",
            "Epoch 7 Batch 5050 Loss 0.9522 Accuracy 0.4999\n",
            "Epoch 7 Batch 5100 Loss 0.9534 Accuracy 0.4997\n",
            "Epoch 7 Batch 5150 Loss 0.9547 Accuracy 0.4995\n",
            "Epoch 7 Batch 5200 Loss 0.9559 Accuracy 0.4992\n",
            "Epoch 7 Batch 5250 Loss 0.9570 Accuracy 0.4989\n",
            "Epoch 7 Batch 5300 Loss 0.9580 Accuracy 0.4986\n",
            "Epoch 7 Batch 5350 Loss 0.9592 Accuracy 0.4983\n",
            "Epoch 7 Batch 5400 Loss 0.9605 Accuracy 0.4980\n",
            "Epoch 7 Batch 5450 Loss 0.9615 Accuracy 0.4978\n",
            "Epoch 7 Batch 5500 Loss 0.9625 Accuracy 0.4975\n",
            "Epoch 7 Batch 5550 Loss 0.9636 Accuracy 0.4973\n",
            "Epoch 7 Batch 5600 Loss 0.9645 Accuracy 0.4971\n",
            "Epoch 7 Batch 5650 Loss 0.9653 Accuracy 0.4969\n",
            "Epoch 7 Batch 5700 Loss 0.9660 Accuracy 0.4966\n",
            "Saving checkpoint for epoch 7 at ./drive/My Drive/projects/transformer/ckpt/ckpt-7\n",
            "Time taken for 1 epoch: 2028.902992963791 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 1.0726 Accuracy 0.4778\n",
            "Epoch 8 Batch 50 Loss 1.0599 Accuracy 0.4796\n",
            "Epoch 8 Batch 100 Loss 1.0508 Accuracy 0.4811\n",
            "Epoch 8 Batch 150 Loss 1.0532 Accuracy 0.4801\n",
            "Epoch 8 Batch 200 Loss 1.0644 Accuracy 0.4803\n",
            "Epoch 8 Batch 250 Loss 1.0655 Accuracy 0.4808\n",
            "Epoch 8 Batch 300 Loss 1.0655 Accuracy 0.4810\n",
            "Epoch 8 Batch 350 Loss 1.0614 Accuracy 0.4808\n",
            "Epoch 8 Batch 400 Loss 1.0591 Accuracy 0.4811\n",
            "Epoch 8 Batch 450 Loss 1.0553 Accuracy 0.4813\n",
            "Epoch 8 Batch 500 Loss 1.0542 Accuracy 0.4817\n",
            "Epoch 8 Batch 550 Loss 1.0507 Accuracy 0.4816\n",
            "Epoch 8 Batch 600 Loss 1.0477 Accuracy 0.4814\n",
            "Epoch 8 Batch 650 Loss 1.0484 Accuracy 0.4814\n",
            "Epoch 8 Batch 700 Loss 1.0463 Accuracy 0.4814\n",
            "Epoch 8 Batch 750 Loss 1.0468 Accuracy 0.4818\n",
            "Epoch 8 Batch 800 Loss 1.0465 Accuracy 0.4820\n",
            "Epoch 8 Batch 850 Loss 1.0456 Accuracy 0.4821\n",
            "Epoch 8 Batch 900 Loss 1.0448 Accuracy 0.4821\n",
            "Epoch 8 Batch 950 Loss 1.0429 Accuracy 0.4825\n",
            "Epoch 8 Batch 1000 Loss 1.0401 Accuracy 0.4827\n",
            "Epoch 8 Batch 1050 Loss 1.0400 Accuracy 0.4829\n",
            "Epoch 8 Batch 1100 Loss 1.0388 Accuracy 0.4829\n",
            "Epoch 8 Batch 1150 Loss 1.0367 Accuracy 0.4831\n",
            "Epoch 8 Batch 1200 Loss 1.0347 Accuracy 0.4835\n",
            "Epoch 8 Batch 1250 Loss 1.0331 Accuracy 0.4837\n",
            "Epoch 8 Batch 1300 Loss 1.0311 Accuracy 0.4842\n",
            "Epoch 8 Batch 1350 Loss 1.0288 Accuracy 0.4848\n",
            "Epoch 8 Batch 1400 Loss 1.0272 Accuracy 0.4854\n",
            "Epoch 8 Batch 1450 Loss 1.0239 Accuracy 0.4863\n",
            "Epoch 8 Batch 1500 Loss 1.0215 Accuracy 0.4871\n",
            "Epoch 8 Batch 1550 Loss 1.0189 Accuracy 0.4880\n",
            "Epoch 8 Batch 1600 Loss 1.0160 Accuracy 0.4888\n",
            "Epoch 8 Batch 1650 Loss 1.0135 Accuracy 0.4895\n",
            "Epoch 8 Batch 1700 Loss 1.0104 Accuracy 0.4903\n",
            "Epoch 8 Batch 1750 Loss 1.0080 Accuracy 0.4909\n",
            "Epoch 8 Batch 1800 Loss 1.0059 Accuracy 0.4916\n",
            "Epoch 8 Batch 1850 Loss 1.0043 Accuracy 0.4924\n",
            "Epoch 8 Batch 1900 Loss 1.0017 Accuracy 0.4931\n",
            "Epoch 8 Batch 1950 Loss 0.9997 Accuracy 0.4937\n",
            "Epoch 8 Batch 2000 Loss 0.9977 Accuracy 0.4945\n",
            "Epoch 8 Batch 2050 Loss 0.9954 Accuracy 0.4949\n",
            "Epoch 8 Batch 2100 Loss 0.9931 Accuracy 0.4954\n",
            "Epoch 8 Batch 2150 Loss 0.9909 Accuracy 0.4957\n",
            "Epoch 8 Batch 2200 Loss 0.9877 Accuracy 0.4960\n",
            "Epoch 8 Batch 2250 Loss 0.9844 Accuracy 0.4960\n",
            "Epoch 8 Batch 2300 Loss 0.9812 Accuracy 0.4962\n",
            "Epoch 8 Batch 2350 Loss 0.9783 Accuracy 0.4963\n",
            "Epoch 8 Batch 2400 Loss 0.9755 Accuracy 0.4965\n",
            "Epoch 8 Batch 2450 Loss 0.9730 Accuracy 0.4968\n",
            "Epoch 8 Batch 2500 Loss 0.9700 Accuracy 0.4971\n",
            "Epoch 8 Batch 2550 Loss 0.9673 Accuracy 0.4974\n",
            "Epoch 8 Batch 2600 Loss 0.9645 Accuracy 0.4977\n",
            "Epoch 8 Batch 2650 Loss 0.9615 Accuracy 0.4980\n",
            "Epoch 8 Batch 2700 Loss 0.9593 Accuracy 0.4984\n",
            "Epoch 8 Batch 2750 Loss 0.9567 Accuracy 0.4988\n",
            "Epoch 8 Batch 2800 Loss 0.9547 Accuracy 0.4991\n",
            "Epoch 8 Batch 2850 Loss 0.9526 Accuracy 0.4993\n",
            "Epoch 8 Batch 2900 Loss 0.9507 Accuracy 0.4996\n",
            "Epoch 8 Batch 2950 Loss 0.9487 Accuracy 0.4998\n",
            "Epoch 8 Batch 3000 Loss 0.9471 Accuracy 0.5001\n",
            "Epoch 8 Batch 3050 Loss 0.9455 Accuracy 0.5004\n",
            "Epoch 8 Batch 3100 Loss 0.9436 Accuracy 0.5007\n",
            "Epoch 8 Batch 3150 Loss 0.9418 Accuracy 0.5009\n",
            "Epoch 8 Batch 3200 Loss 0.9397 Accuracy 0.5011\n",
            "Epoch 8 Batch 3250 Loss 0.9376 Accuracy 0.5013\n",
            "Epoch 8 Batch 3300 Loss 0.9357 Accuracy 0.5016\n",
            "Epoch 8 Batch 3350 Loss 0.9339 Accuracy 0.5019\n",
            "Epoch 8 Batch 3400 Loss 0.9320 Accuracy 0.5022\n",
            "Epoch 8 Batch 3450 Loss 0.9300 Accuracy 0.5025\n",
            "Epoch 8 Batch 3500 Loss 0.9285 Accuracy 0.5028\n",
            "Epoch 8 Batch 3550 Loss 0.9266 Accuracy 0.5031\n",
            "Epoch 8 Batch 3600 Loss 0.9248 Accuracy 0.5033\n",
            "Epoch 8 Batch 3650 Loss 0.9231 Accuracy 0.5037\n",
            "Epoch 8 Batch 3700 Loss 0.9215 Accuracy 0.5039\n",
            "Epoch 8 Batch 3750 Loss 0.9197 Accuracy 0.5043\n",
            "Epoch 8 Batch 3800 Loss 0.9182 Accuracy 0.5046\n",
            "Epoch 8 Batch 3850 Loss 0.9171 Accuracy 0.5049\n",
            "Epoch 8 Batch 3900 Loss 0.9156 Accuracy 0.5052\n",
            "Epoch 8 Batch 3950 Loss 0.9143 Accuracy 0.5056\n",
            "Epoch 8 Batch 4000 Loss 0.9131 Accuracy 0.5059\n",
            "Epoch 8 Batch 4050 Loss 0.9119 Accuracy 0.5062\n",
            "Epoch 8 Batch 4100 Loss 0.9111 Accuracy 0.5064\n",
            "Epoch 8 Batch 4150 Loss 0.9107 Accuracy 0.5065\n",
            "Epoch 8 Batch 4200 Loss 0.9109 Accuracy 0.5065\n",
            "Epoch 8 Batch 4250 Loss 0.9114 Accuracy 0.5065\n",
            "Epoch 8 Batch 4300 Loss 0.9122 Accuracy 0.5064\n",
            "Epoch 8 Batch 4350 Loss 0.9134 Accuracy 0.5063\n",
            "Epoch 8 Batch 4400 Loss 0.9143 Accuracy 0.5061\n",
            "Epoch 8 Batch 4450 Loss 0.9152 Accuracy 0.5059\n",
            "Epoch 8 Batch 4500 Loss 0.9165 Accuracy 0.5058\n",
            "Epoch 8 Batch 4550 Loss 0.9182 Accuracy 0.5056\n",
            "Epoch 8 Batch 4600 Loss 0.9196 Accuracy 0.5054\n",
            "Epoch 8 Batch 4650 Loss 0.9209 Accuracy 0.5051\n",
            "Epoch 8 Batch 4700 Loss 0.9225 Accuracy 0.5049\n",
            "Epoch 8 Batch 4750 Loss 0.9238 Accuracy 0.5048\n",
            "Epoch 8 Batch 4800 Loss 0.9248 Accuracy 0.5046\n",
            "Epoch 8 Batch 4850 Loss 0.9260 Accuracy 0.5045\n",
            "Epoch 8 Batch 4900 Loss 0.9273 Accuracy 0.5043\n",
            "Epoch 8 Batch 4950 Loss 0.9285 Accuracy 0.5041\n",
            "Epoch 8 Batch 5000 Loss 0.9297 Accuracy 0.5039\n",
            "Epoch 8 Batch 5050 Loss 0.9310 Accuracy 0.5037\n",
            "Epoch 8 Batch 5100 Loss 0.9320 Accuracy 0.5034\n",
            "Epoch 8 Batch 5150 Loss 0.9333 Accuracy 0.5032\n",
            "Epoch 8 Batch 5200 Loss 0.9345 Accuracy 0.5029\n",
            "Epoch 8 Batch 5250 Loss 0.9356 Accuracy 0.5026\n",
            "Epoch 8 Batch 5300 Loss 0.9368 Accuracy 0.5023\n",
            "Epoch 8 Batch 5350 Loss 0.9380 Accuracy 0.5020\n",
            "Epoch 8 Batch 5400 Loss 0.9393 Accuracy 0.5017\n",
            "Epoch 8 Batch 5450 Loss 0.9403 Accuracy 0.5015\n",
            "Epoch 8 Batch 5500 Loss 0.9412 Accuracy 0.5012\n",
            "Epoch 8 Batch 5550 Loss 0.9422 Accuracy 0.5010\n",
            "Epoch 8 Batch 5600 Loss 0.9430 Accuracy 0.5008\n",
            "Epoch 8 Batch 5650 Loss 0.9439 Accuracy 0.5005\n",
            "Epoch 8 Batch 5700 Loss 0.9449 Accuracy 0.5003\n",
            "Saving checkpoint for epoch 8 at ./drive/My Drive/projects/transformer/ckpt/ckpt-8\n",
            "Time taken for 1 epoch: 2030.4735114574432 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 1.0974 Accuracy 0.4918\n",
            "Epoch 9 Batch 50 Loss 1.0426 Accuracy 0.4821\n",
            "Epoch 9 Batch 100 Loss 1.0371 Accuracy 0.4843\n",
            "Epoch 9 Batch 150 Loss 1.0380 Accuracy 0.4862\n",
            "Epoch 9 Batch 200 Loss 1.0372 Accuracy 0.4854\n",
            "Epoch 9 Batch 250 Loss 1.0374 Accuracy 0.4856\n",
            "Epoch 9 Batch 300 Loss 1.0368 Accuracy 0.4850\n",
            "Epoch 9 Batch 350 Loss 1.0369 Accuracy 0.4849\n",
            "Epoch 9 Batch 400 Loss 1.0339 Accuracy 0.4850\n",
            "Epoch 9 Batch 450 Loss 1.0317 Accuracy 0.4844\n",
            "Epoch 9 Batch 500 Loss 1.0292 Accuracy 0.4846\n",
            "Epoch 9 Batch 550 Loss 1.0292 Accuracy 0.4844\n",
            "Epoch 9 Batch 600 Loss 1.0306 Accuracy 0.4841\n",
            "Epoch 9 Batch 650 Loss 1.0305 Accuracy 0.4845\n",
            "Epoch 9 Batch 700 Loss 1.0295 Accuracy 0.4847\n",
            "Epoch 9 Batch 750 Loss 1.0278 Accuracy 0.4848\n",
            "Epoch 9 Batch 800 Loss 1.0263 Accuracy 0.4850\n",
            "Epoch 9 Batch 850 Loss 1.0245 Accuracy 0.4854\n",
            "Epoch 9 Batch 900 Loss 1.0238 Accuracy 0.4859\n",
            "Epoch 9 Batch 950 Loss 1.0223 Accuracy 0.4861\n",
            "Epoch 9 Batch 1000 Loss 1.0200 Accuracy 0.4859\n",
            "Epoch 9 Batch 1050 Loss 1.0173 Accuracy 0.4859\n",
            "Epoch 9 Batch 1100 Loss 1.0165 Accuracy 0.4858\n",
            "Epoch 9 Batch 1150 Loss 1.0153 Accuracy 0.4861\n",
            "Epoch 9 Batch 1200 Loss 1.0137 Accuracy 0.4862\n",
            "Epoch 9 Batch 1250 Loss 1.0113 Accuracy 0.4864\n",
            "Epoch 9 Batch 1300 Loss 1.0097 Accuracy 0.4867\n",
            "Epoch 9 Batch 1350 Loss 1.0084 Accuracy 0.4873\n",
            "Epoch 9 Batch 1400 Loss 1.0057 Accuracy 0.4880\n",
            "Epoch 9 Batch 1450 Loss 1.0032 Accuracy 0.4888\n",
            "Epoch 9 Batch 1500 Loss 1.0006 Accuracy 0.4898\n",
            "Epoch 9 Batch 1550 Loss 0.9985 Accuracy 0.4907\n",
            "Epoch 9 Batch 1600 Loss 0.9965 Accuracy 0.4913\n",
            "Epoch 9 Batch 1650 Loss 0.9936 Accuracy 0.4921\n",
            "Epoch 9 Batch 1700 Loss 0.9909 Accuracy 0.4930\n",
            "Epoch 9 Batch 1750 Loss 0.9885 Accuracy 0.4937\n",
            "Epoch 9 Batch 1800 Loss 0.9856 Accuracy 0.4944\n",
            "Epoch 9 Batch 1850 Loss 0.9836 Accuracy 0.4953\n",
            "Epoch 9 Batch 1900 Loss 0.9814 Accuracy 0.4960\n",
            "Epoch 9 Batch 1950 Loss 0.9794 Accuracy 0.4968\n",
            "Epoch 9 Batch 2000 Loss 0.9775 Accuracy 0.4973\n",
            "Epoch 9 Batch 2050 Loss 0.9757 Accuracy 0.4977\n",
            "Epoch 9 Batch 2100 Loss 0.9728 Accuracy 0.4982\n",
            "Epoch 9 Batch 2150 Loss 0.9704 Accuracy 0.4986\n",
            "Epoch 9 Batch 2200 Loss 0.9678 Accuracy 0.4988\n",
            "Epoch 9 Batch 2250 Loss 0.9647 Accuracy 0.4989\n",
            "Epoch 9 Batch 2300 Loss 0.9625 Accuracy 0.4991\n",
            "Epoch 9 Batch 2350 Loss 0.9597 Accuracy 0.4993\n",
            "Epoch 9 Batch 2400 Loss 0.9568 Accuracy 0.4995\n",
            "Epoch 9 Batch 2450 Loss 0.9537 Accuracy 0.4997\n",
            "Epoch 9 Batch 2500 Loss 0.9512 Accuracy 0.4998\n",
            "Epoch 9 Batch 2550 Loss 0.9484 Accuracy 0.5002\n",
            "Epoch 9 Batch 2600 Loss 0.9459 Accuracy 0.5005\n",
            "Epoch 9 Batch 2650 Loss 0.9433 Accuracy 0.5008\n",
            "Epoch 9 Batch 2700 Loss 0.9409 Accuracy 0.5012\n",
            "Epoch 9 Batch 2750 Loss 0.9385 Accuracy 0.5016\n",
            "Epoch 9 Batch 2800 Loss 0.9368 Accuracy 0.5019\n",
            "Epoch 9 Batch 2850 Loss 0.9345 Accuracy 0.5021\n",
            "Epoch 9 Batch 2900 Loss 0.9324 Accuracy 0.5025\n",
            "Epoch 9 Batch 2950 Loss 0.9301 Accuracy 0.5027\n",
            "Epoch 9 Batch 3000 Loss 0.9280 Accuracy 0.5029\n",
            "Epoch 9 Batch 3050 Loss 0.9262 Accuracy 0.5031\n",
            "Epoch 9 Batch 3100 Loss 0.9242 Accuracy 0.5033\n",
            "Epoch 9 Batch 3150 Loss 0.9227 Accuracy 0.5036\n",
            "Epoch 9 Batch 3200 Loss 0.9210 Accuracy 0.5038\n",
            "Epoch 9 Batch 3250 Loss 0.9193 Accuracy 0.5041\n",
            "Epoch 9 Batch 3300 Loss 0.9175 Accuracy 0.5044\n",
            "Epoch 9 Batch 3350 Loss 0.9158 Accuracy 0.5046\n",
            "Epoch 9 Batch 3400 Loss 0.9140 Accuracy 0.5049\n",
            "Epoch 9 Batch 3450 Loss 0.9119 Accuracy 0.5053\n",
            "Epoch 9 Batch 3500 Loss 0.9101 Accuracy 0.5056\n",
            "Epoch 9 Batch 3550 Loss 0.9085 Accuracy 0.5059\n",
            "Epoch 9 Batch 3600 Loss 0.9067 Accuracy 0.5061\n",
            "Epoch 9 Batch 3650 Loss 0.9053 Accuracy 0.5064\n",
            "Epoch 9 Batch 3700 Loss 0.9036 Accuracy 0.5067\n",
            "Epoch 9 Batch 3750 Loss 0.9022 Accuracy 0.5071\n",
            "Epoch 9 Batch 3800 Loss 0.9003 Accuracy 0.5074\n",
            "Epoch 9 Batch 3850 Loss 0.8990 Accuracy 0.5078\n",
            "Epoch 9 Batch 3900 Loss 0.8976 Accuracy 0.5081\n",
            "Epoch 9 Batch 3950 Loss 0.8963 Accuracy 0.5084\n",
            "Epoch 9 Batch 4000 Loss 0.8949 Accuracy 0.5087\n",
            "Epoch 9 Batch 4050 Loss 0.8939 Accuracy 0.5091\n",
            "Epoch 9 Batch 4100 Loss 0.8929 Accuracy 0.5093\n",
            "Epoch 9 Batch 4150 Loss 0.8925 Accuracy 0.5094\n",
            "Epoch 9 Batch 4200 Loss 0.8927 Accuracy 0.5094\n",
            "Epoch 9 Batch 4250 Loss 0.8931 Accuracy 0.5093\n",
            "Epoch 9 Batch 4300 Loss 0.8939 Accuracy 0.5092\n",
            "Epoch 9 Batch 4350 Loss 0.8950 Accuracy 0.5091\n",
            "Epoch 9 Batch 4400 Loss 0.8962 Accuracy 0.5089\n",
            "Epoch 9 Batch 4450 Loss 0.8975 Accuracy 0.5087\n",
            "Epoch 9 Batch 4500 Loss 0.8988 Accuracy 0.5086\n",
            "Epoch 9 Batch 4550 Loss 0.9002 Accuracy 0.5084\n",
            "Epoch 9 Batch 4600 Loss 0.9016 Accuracy 0.5082\n",
            "Epoch 9 Batch 4650 Loss 0.9029 Accuracy 0.5080\n",
            "Epoch 9 Batch 4700 Loss 0.9045 Accuracy 0.5078\n",
            "Epoch 9 Batch 4750 Loss 0.9059 Accuracy 0.5076\n",
            "Epoch 9 Batch 4800 Loss 0.9073 Accuracy 0.5075\n",
            "Epoch 9 Batch 4850 Loss 0.9084 Accuracy 0.5073\n",
            "Epoch 9 Batch 4900 Loss 0.9096 Accuracy 0.5071\n",
            "Epoch 9 Batch 4950 Loss 0.9112 Accuracy 0.5069\n",
            "Epoch 9 Batch 5000 Loss 0.9123 Accuracy 0.5067\n",
            "Epoch 9 Batch 5050 Loss 0.9138 Accuracy 0.5064\n",
            "Epoch 9 Batch 5100 Loss 0.9150 Accuracy 0.5062\n",
            "Epoch 9 Batch 5150 Loss 0.9162 Accuracy 0.5060\n",
            "Epoch 9 Batch 5200 Loss 0.9177 Accuracy 0.5057\n",
            "Epoch 9 Batch 5250 Loss 0.9188 Accuracy 0.5055\n",
            "Epoch 9 Batch 5300 Loss 0.9199 Accuracy 0.5051\n",
            "Epoch 9 Batch 5350 Loss 0.9212 Accuracy 0.5048\n",
            "Epoch 9 Batch 5400 Loss 0.9224 Accuracy 0.5046\n",
            "Epoch 9 Batch 5450 Loss 0.9234 Accuracy 0.5043\n",
            "Epoch 9 Batch 5500 Loss 0.9243 Accuracy 0.5041\n",
            "Epoch 9 Batch 5550 Loss 0.9252 Accuracy 0.5037\n",
            "Epoch 9 Batch 5600 Loss 0.9261 Accuracy 0.5035\n",
            "Epoch 9 Batch 5650 Loss 0.9271 Accuracy 0.5033\n",
            "Epoch 9 Batch 5700 Loss 0.9280 Accuracy 0.5030\n",
            "Saving checkpoint for epoch 9 at ./drive/My Drive/projects/transformer/ckpt/ckpt-9\n",
            "Time taken for 1 epoch: 2008.691546678543 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 0.9155 Accuracy 0.5288\n",
            "Epoch 10 Batch 50 Loss 1.0106 Accuracy 0.4927\n",
            "Epoch 10 Batch 100 Loss 1.0120 Accuracy 0.4908\n",
            "Epoch 10 Batch 150 Loss 1.0166 Accuracy 0.4883\n",
            "Epoch 10 Batch 200 Loss 1.0166 Accuracy 0.4879\n",
            "Epoch 10 Batch 250 Loss 1.0199 Accuracy 0.4874\n",
            "Epoch 10 Batch 300 Loss 1.0151 Accuracy 0.4873\n",
            "Epoch 10 Batch 350 Loss 1.0134 Accuracy 0.4866\n",
            "Epoch 10 Batch 400 Loss 1.0109 Accuracy 0.4874\n",
            "Epoch 10 Batch 450 Loss 1.0102 Accuracy 0.4867\n",
            "Epoch 10 Batch 500 Loss 1.0081 Accuracy 0.4870\n",
            "Epoch 10 Batch 550 Loss 1.0084 Accuracy 0.4871\n",
            "Epoch 10 Batch 600 Loss 1.0092 Accuracy 0.4873\n",
            "Epoch 10 Batch 650 Loss 1.0097 Accuracy 0.4879\n",
            "Epoch 10 Batch 700 Loss 1.0089 Accuracy 0.4882\n",
            "Epoch 10 Batch 750 Loss 1.0102 Accuracy 0.4883\n",
            "Epoch 10 Batch 800 Loss 1.0100 Accuracy 0.4885\n",
            "Epoch 10 Batch 850 Loss 1.0082 Accuracy 0.4886\n",
            "Epoch 10 Batch 900 Loss 1.0075 Accuracy 0.4887\n",
            "Epoch 10 Batch 950 Loss 1.0066 Accuracy 0.4885\n",
            "Epoch 10 Batch 1000 Loss 1.0044 Accuracy 0.4883\n",
            "Epoch 10 Batch 1050 Loss 1.0023 Accuracy 0.4883\n",
            "Epoch 10 Batch 1100 Loss 1.0014 Accuracy 0.4884\n",
            "Epoch 10 Batch 1150 Loss 1.0004 Accuracy 0.4888\n",
            "Epoch 10 Batch 1200 Loss 0.9982 Accuracy 0.4892\n",
            "Epoch 10 Batch 1250 Loss 0.9962 Accuracy 0.4896\n",
            "Epoch 10 Batch 1300 Loss 0.9943 Accuracy 0.4900\n",
            "Epoch 10 Batch 1350 Loss 0.9925 Accuracy 0.4906\n",
            "Epoch 10 Batch 1400 Loss 0.9897 Accuracy 0.4911\n",
            "Epoch 10 Batch 1450 Loss 0.9881 Accuracy 0.4918\n",
            "Epoch 10 Batch 1500 Loss 0.9855 Accuracy 0.4924\n",
            "Epoch 10 Batch 1550 Loss 0.9831 Accuracy 0.4933\n",
            "Epoch 10 Batch 1600 Loss 0.9799 Accuracy 0.4941\n",
            "Epoch 10 Batch 1650 Loss 0.9775 Accuracy 0.4949\n",
            "Epoch 10 Batch 1700 Loss 0.9749 Accuracy 0.4957\n",
            "Epoch 10 Batch 1750 Loss 0.9730 Accuracy 0.4964\n",
            "Epoch 10 Batch 1800 Loss 0.9711 Accuracy 0.4971\n",
            "Epoch 10 Batch 1850 Loss 0.9686 Accuracy 0.4978\n",
            "Epoch 10 Batch 1900 Loss 0.9662 Accuracy 0.4986\n",
            "Epoch 10 Batch 1950 Loss 0.9639 Accuracy 0.4994\n",
            "Epoch 10 Batch 2000 Loss 0.9625 Accuracy 0.4999\n",
            "Epoch 10 Batch 2050 Loss 0.9608 Accuracy 0.5003\n",
            "Epoch 10 Batch 2100 Loss 0.9585 Accuracy 0.5008\n",
            "Epoch 10 Batch 2150 Loss 0.9557 Accuracy 0.5010\n",
            "Epoch 10 Batch 2200 Loss 0.9530 Accuracy 0.5012\n",
            "Epoch 10 Batch 2250 Loss 0.9501 Accuracy 0.5013\n",
            "Epoch 10 Batch 2300 Loss 0.9471 Accuracy 0.5015\n",
            "Epoch 10 Batch 2350 Loss 0.9440 Accuracy 0.5018\n",
            "Epoch 10 Batch 2400 Loss 0.9413 Accuracy 0.5021\n",
            "Epoch 10 Batch 2450 Loss 0.9384 Accuracy 0.5024\n",
            "Epoch 10 Batch 2500 Loss 0.9354 Accuracy 0.5028\n",
            "Epoch 10 Batch 2550 Loss 0.9328 Accuracy 0.5031\n",
            "Epoch 10 Batch 2600 Loss 0.9305 Accuracy 0.5034\n",
            "Epoch 10 Batch 2650 Loss 0.9281 Accuracy 0.5038\n",
            "Epoch 10 Batch 2700 Loss 0.9256 Accuracy 0.5040\n",
            "Epoch 10 Batch 2750 Loss 0.9234 Accuracy 0.5043\n",
            "Epoch 10 Batch 2800 Loss 0.9212 Accuracy 0.5046\n",
            "Epoch 10 Batch 2850 Loss 0.9189 Accuracy 0.5048\n",
            "Epoch 10 Batch 2900 Loss 0.9170 Accuracy 0.5051\n",
            "Epoch 10 Batch 2950 Loss 0.9152 Accuracy 0.5054\n",
            "Epoch 10 Batch 3000 Loss 0.9129 Accuracy 0.5056\n",
            "Epoch 10 Batch 3050 Loss 0.9111 Accuracy 0.5059\n",
            "Epoch 10 Batch 3100 Loss 0.9094 Accuracy 0.5061\n",
            "Epoch 10 Batch 3150 Loss 0.9076 Accuracy 0.5064\n",
            "Epoch 10 Batch 3200 Loss 0.9056 Accuracy 0.5066\n",
            "Epoch 10 Batch 3250 Loss 0.9037 Accuracy 0.5068\n",
            "Epoch 10 Batch 3300 Loss 0.9016 Accuracy 0.5071\n",
            "Epoch 10 Batch 3350 Loss 0.9000 Accuracy 0.5074\n",
            "Epoch 10 Batch 3400 Loss 0.8982 Accuracy 0.5077\n",
            "Epoch 10 Batch 3450 Loss 0.8967 Accuracy 0.5080\n",
            "Epoch 10 Batch 3500 Loss 0.8949 Accuracy 0.5082\n",
            "Epoch 10 Batch 3550 Loss 0.8934 Accuracy 0.5085\n",
            "Epoch 10 Batch 3600 Loss 0.8915 Accuracy 0.5088\n",
            "Epoch 10 Batch 3650 Loss 0.8897 Accuracy 0.5091\n",
            "Epoch 10 Batch 3700 Loss 0.8880 Accuracy 0.5093\n",
            "Epoch 10 Batch 3750 Loss 0.8866 Accuracy 0.5096\n",
            "Epoch 10 Batch 3800 Loss 0.8855 Accuracy 0.5100\n",
            "Epoch 10 Batch 3850 Loss 0.8840 Accuracy 0.5103\n",
            "Epoch 10 Batch 3900 Loss 0.8827 Accuracy 0.5106\n",
            "Epoch 10 Batch 3950 Loss 0.8814 Accuracy 0.5110\n",
            "Epoch 10 Batch 4000 Loss 0.8801 Accuracy 0.5112\n",
            "Epoch 10 Batch 4050 Loss 0.8790 Accuracy 0.5115\n",
            "Epoch 10 Batch 4100 Loss 0.8780 Accuracy 0.5118\n",
            "Epoch 10 Batch 4150 Loss 0.8777 Accuracy 0.5118\n",
            "Epoch 10 Batch 4200 Loss 0.8780 Accuracy 0.5118\n",
            "Epoch 10 Batch 4250 Loss 0.8783 Accuracy 0.5118\n",
            "Epoch 10 Batch 4300 Loss 0.8792 Accuracy 0.5118\n",
            "Epoch 10 Batch 4350 Loss 0.8802 Accuracy 0.5116\n",
            "Epoch 10 Batch 4400 Loss 0.8813 Accuracy 0.5114\n",
            "Epoch 10 Batch 4450 Loss 0.8824 Accuracy 0.5112\n",
            "Epoch 10 Batch 4500 Loss 0.8837 Accuracy 0.5110\n",
            "Epoch 10 Batch 4550 Loss 0.8852 Accuracy 0.5108\n",
            "Epoch 10 Batch 4600 Loss 0.8866 Accuracy 0.5107\n",
            "Epoch 10 Batch 4650 Loss 0.8878 Accuracy 0.5104\n",
            "Epoch 10 Batch 4700 Loss 0.8891 Accuracy 0.5103\n",
            "Epoch 10 Batch 4750 Loss 0.8905 Accuracy 0.5101\n",
            "Epoch 10 Batch 4800 Loss 0.8918 Accuracy 0.5098\n",
            "Epoch 10 Batch 4850 Loss 0.8930 Accuracy 0.5097\n",
            "Epoch 10 Batch 4900 Loss 0.8944 Accuracy 0.5095\n",
            "Epoch 10 Batch 4950 Loss 0.8955 Accuracy 0.5092\n",
            "Epoch 10 Batch 5000 Loss 0.8970 Accuracy 0.5090\n",
            "Epoch 10 Batch 5050 Loss 0.8982 Accuracy 0.5088\n",
            "Epoch 10 Batch 5100 Loss 0.8997 Accuracy 0.5086\n",
            "Epoch 10 Batch 5150 Loss 0.9011 Accuracy 0.5084\n",
            "Epoch 10 Batch 5200 Loss 0.9024 Accuracy 0.5081\n",
            "Epoch 10 Batch 5250 Loss 0.9035 Accuracy 0.5078\n",
            "Epoch 10 Batch 5300 Loss 0.9046 Accuracy 0.5076\n",
            "Epoch 10 Batch 5350 Loss 0.9057 Accuracy 0.5072\n",
            "Epoch 10 Batch 5400 Loss 0.9067 Accuracy 0.5069\n",
            "Epoch 10 Batch 5450 Loss 0.9079 Accuracy 0.5067\n",
            "Epoch 10 Batch 5500 Loss 0.9092 Accuracy 0.5064\n",
            "Epoch 10 Batch 5550 Loss 0.9102 Accuracy 0.5061\n",
            "Epoch 10 Batch 5600 Loss 0.9112 Accuracy 0.5059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmzyRwDrRGdq",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNHwJJrz3lPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6VeFKrE6Kdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdoWKbCP7Czs",
        "colab_type": "code",
        "outputId": "91523420-98f6-4c0d-b336-643cf6f43308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate(\"This is a really powerful tool!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: This is a really powerful tool!\n",
            "Predicted translation: C'est un instrument vraiment puissant!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}